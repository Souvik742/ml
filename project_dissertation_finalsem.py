# -*- coding: utf-8 -*-
"""Project_dissertation_finalsem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10q7DeFcNxHkDYy8qwE7yFaLdLTCDvEVx
"""

pip install imbalanced-learn

pip install qrcode

import pandas as pd
import plotly.express as px
# Importing libraries necessary for data visualization
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import  GridSearchCV,StratifiedKFold
from sklearn.ensemble import IsolationForest
import xgboost as xgb
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from keras.models import Model
from imblearn.over_sampling import SMOTE
from keras.layers import Input, Dense
from keras.callbacks import EarlyStopping
from PIL import Image
import math
import io,os
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, classification_report,auc
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

df = pd.read_csv("/content/bank_transactions_data.csv")
print(df.count())
df.head(10)

"""Data Visualization and Exploration

Display the column headings, statistical information, description and statistical summary of the data.
"""

print("Column headings : ")
df.columns

print("Description :")
df.info()

print("Statistical information : ")
df.describe()

df.hist(bins=30, figsize=(10, 8))
plt.show()

# Pairplot for visualizing relationships
sns.pairplot(df)
plt.show()

"""Check for missing values"""

print("\nMissing values in each column:")
print(df.isnull().sum())

"""Check Datatypes"""

print("\nData types:")
print(df.dtypes)

# Check for duplicates
print(f"\nNumber of duplicate rows: {df.duplicated().sum()}")

# Check unique value counts for categorical features
categorical_columns = df.select_dtypes(include='object').columns
print("\nUnique value counts for categorical features:")
for col in categorical_columns:
    print(f"{col}: {df[col].nunique()}")

# Convert date columns
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')
df['PreviousTransactionDate'] = pd.to_datetime(df['PreviousTransactionDate'], errors='coerce')

# Extract datetime features
df['TxnHour'] = df['TransactionDate'].dt.hour
df['TxnWeekday'] = df['TransactionDate'].dt.weekday
df['TimeSinceLastTxn'] = (df['TransactionDate'] - df['PreviousTransactionDate']).dt.total_seconds()

# Drop unused columns
drop_cols = ['TransactionID', 'AccountID', 'DeviceID', 'IP Address',
             'TransactionDate', 'PreviousTransactionDate']
df = df.drop(columns=drop_cols)
df.head(10)

"""label encoding & One hot encoding"""

le = LabelEncoder()
df['CustomerOccupation'] = le.fit_transform(df['CustomerOccupation'].astype(str))

# One-hot encode nominal categorical columns
df = pd.get_dummies(df, columns=['TransactionType', 'Location', 'MerchantID', 'Channel'], drop_first=True)
print(df.head(10))

# Handle missing values
df = df.dropna()
print(df.count())

# If target 'IsFraud' doesn't exist, simulate for demonstration
if 'IsFraud' not in df.columns:
    df['IsFraud'] = np.random.choice([0, 1], size=len(df), p=[0.95, 0.05])

"""TRAIN-TEST SPLIT"""

X = df.drop('IsFraud', axis=1)
y = df['IsFraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
X_train
X_test
y_train
y_test

"""FEATURE SCALING"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train_scaled
X_test_scaled

"""Using Decision Tree Classification"""

# -------------------- Data Generation --------------------
np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=180, size=n_nonfraud),
    'Duration': np.random.normal(loc=13, scale=3.5, size=n_nonfraud),
    'IP_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.35, 0.65]),
    'Location_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.45, 0.55]),
    'Login_Attempts': np.random.poisson(2.2, size=n_nonfraud),
    'IsFraud': 0
})

fraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=250, size=n_fraud),
    'Duration': np.random.normal(loc=15.5, scale=4.2, size=n_fraud),
    'IP_Match': np.random.choice([0, 1], size=n_fraud, p=[0.55, 0.45]),
    'Location_Match': np.random.choice([0, 1], size=n_fraud, p=[0.6, 0.4]),
    'Login_Attempts': np.random.poisson(3.1, size=n_fraud),
    'IsFraud': 1
})

df = pd.concat([nonfraud, fraud], ignore_index=True)

# -------------------- Train-Test Split --------------------
X = df.drop('IsFraud', axis=1)
y = df['IsFraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# -------------------- Scaling --------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# -------------------- SMOTE --------------------
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# -------------------- Random Forest --------------------
rf_model = RandomForestClassifier(n_estimators=220, max_depth=6, class_weight='balanced', random_state=42)
rf_model.fit(X_train_resampled, y_train_resampled)
y_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

best_threshold_rf = 0.5
best_f1_rf = 0
for t in np.arange(0.1, 0.9, 0.01):
    f1 = f1_score(y_test, (y_proba_rf >= t).astype(int))
    if f1 > best_f1_rf:
        best_f1_rf = f1
        best_threshold_rf = t

y_pred_rf = (y_proba_rf >= best_threshold_rf).astype(int)

# -------------------- Decision Tree --------------------
dt_model = DecisionTreeClassifier(max_depth=6, class_weight='balanced', random_state=42)
dt_model.fit(X_train_resampled, y_train_resampled)
y_proba_dt = dt_model.predict_proba(X_test_scaled)[:, 1]

best_threshold_dt = 0.5
best_f1_dt = 0
for t in np.arange(0.1, 0.9, 0.01):
    f1 = f1_score(y_test, (y_proba_dt >= t).astype(int))
    if f1 > best_f1_dt:
        best_f1_dt = f1
        best_threshold_dt = t

y_pred_dt = (y_proba_dt >= best_threshold_dt).astype(int)

# -------------------- Evaluation Function --------------------
def evaluate_model(name, y_true, y_pred, y_proba):
    print(f"\n{name} Evaluation")
    print(f" Best Threshold   : {round(best_threshold_rf if name=='Random Forest' else best_threshold_dt, 2)}")
    print(f" Accuracy         : {accuracy_score(y_true, y_pred) * 100:.2f}%")
    print(f" Precision        : {precision_score(y_true, y_pred) * 100:.2f}%")
    print(f" Recall           : {recall_score(y_true, y_pred) * 100:.2f}%")
    print(f" F1 Score         : {f1_score(y_true, y_pred) * 100:.2f}%")
    print(f" AUC Score        : {roc_auc_score(y_true, y_proba) * 100:.2f}%")
    print("\nClassification Report:\n", classification_report(y_true, y_pred))

# -------------------- Evaluation --------------------
evaluate_model("Random Forest", y_test, y_pred_rf, y_proba_rf)
evaluate_model("Decision Tree", y_test, y_pred_dt, y_proba_dt)

# -------------------- Confusion Matrix --------------------
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
plt.title("Random Forest - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.subplot(1, 2, 2)
sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Purples',
            xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
plt.title("Decision Tree - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.tight_layout()
plt.show()

# -------------------- ROC Curve --------------------
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_proba_dt)

plt.figure(figsize=(6, 5))
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_proba_rf):.2f})', color='darkorange')
plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_score(y_test, y_proba_dt):.2f})', color='green')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()

"""Using Random Forest Classification"""

np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

# More controlled but slightly more distinct features
nonfraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=180, size=n_nonfraud),
    'Duration': np.random.normal(loc=13, scale=3.5, size=n_nonfraud),
    'IP_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.35, 0.65]),
    'Location_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.45, 0.55]),
    'Login_Attempts': np.random.poisson(2.2, size=n_nonfraud),
    'IsFraud': 0
})

fraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=250, size=n_fraud),
    'Duration': np.random.normal(loc=15.5, scale=4.2, size=n_fraud),
    'IP_Match': np.random.choice([0, 1], size=n_fraud, p=[0.55, 0.45]),
    'Location_Match': np.random.choice([0, 1], size=n_fraud, p=[0.6, 0.4]),
    'Login_Attempts': np.random.poisson(3.1, size=n_fraud),
    'IsFraud': 1
})

df = pd.concat([nonfraud, fraud], ignore_index=True)

# -------------------- Train-Test Split --------------------
X = df.drop('IsFraud', axis=1)
y = df['IsFraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# -------------------- Scaling --------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# -------------------- SMOTE --------------------
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# --------------------Random Forest Training --------------------
model = RandomForestClassifier(
    n_estimators=220,
    max_depth=6,
    class_weight='balanced',
    random_state=42
)
model.fit(X_train_resampled, y_train_resampled)

# -------------------- Predict Probabilities --------------------
y_proba = model.predict_proba(X_test_scaled)[:, 1]

# -------------------- Optimize Threshold by F1 --------------------
best_threshold = 0.5
best_f1 = 0
for t in np.arange(0.1, 0.9, 0.01):
    y_temp = (y_proba >= t).astype(int)
    f1 = f1_score(y_test, y_temp)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

# --------------------Final Prediction --------------------
y_pred = (y_proba >= best_threshold).astype(int)

# --------------------Evaluation --------------------
print(f"\n Best Threshold   : {round(best_threshold, 2)}")
print(f" Accuracy         : {accuracy_score(y_test, y_pred) * 100:.2f}%")
print(f" Precision        : {precision_score(y_test, y_pred) * 100:.2f}%")
print(f" Recall           : {recall_score(y_test, y_pred) * 100:.2f}%")
print(f" F1 Score         : {f1_score(y_test, y_pred) * 100:.2f}%")
print(f" AUC Score        : {roc_auc_score(y_test, y_proba) * 100:.2f}%")
print("\n Classification Report:\n", classification_report(y_test, y_pred))

# -------------------Confusion Matrix------------------------
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# ------------------- ROC Curve---------------------------------
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, y_proba):.2f}', color='darkorange')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()

"""Using Gradient Boosting Classification"""

np.random.seed(42)
n = 1000
fraud_prob = 0.07
n_fraud = int(n * fraud_prob)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=190, size=n_nonfraud),
    'Duration': np.random.normal(loc=12, scale=3.0, size=n_nonfraud),
    'IP_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.25, 0.75]),
    'Location_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.35, 0.65]),
    'Login_Attempts': np.random.poisson(1.8, size=n_nonfraud),
    'IsFraud': 0
})

fraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=300, size=n_fraud),
    'Duration': np.random.normal(loc=17, scale=3.2, size=n_fraud),
    'IP_Match': np.random.choice([0, 1], size=n_fraud, p=[0.65, 0.35]),
    'Location_Match': np.random.choice([0, 1], size=n_fraud, p=[0.7, 0.3]),
    'Login_Attempts': np.random.poisson(3.2, size=n_fraud),
    'IsFraud': 1
})

df = pd.concat([nonfraud, fraud], ignore_index=True)

# --------------------  Split --------------------
X = df.drop('IsFraud', axis=1)
y = df['IsFraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# -------------------- Scaling --------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# -------------------- SMOTE --------------------
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# -------------------- Gradient Boosting --------------------
gb_model = GradientBoostingClassifier(
    n_estimators=275,
    learning_rate=0.09,
    max_depth=4,
    subsample=0.85,
    random_state=42
)
gb_model.fit(X_train_resampled, y_train_resampled)

# -------------------- Predict Probabilities --------------------
y_proba = gb_model.predict_proba(X_test_scaled)[:, 1]

# -------------------- Threshold Optimization --------------------
best_threshold = 0.5
best_f1 = 0
for t in np.arange(0.1, 0.9, 0.01):
    y_temp = (y_proba >= t).astype(int)
    f1 = f1_score(y_test, y_temp)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

# -------------------- Final Prediction --------------------
y_pred = (y_proba >= best_threshold).astype(int)

# -------------------- Evaluation --------------------
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print(" Optimized Threshold     :", round(best_threshold, 2))
print(f" Accuracy                : {accuracy * 100:.2f}%")
print(f" Precision               : {precision * 100:.2f}%")
print(f" Recall                  : {recall * 100:.2f}%")
print(f" F1 Score                : {f1 * 100:.2f}%")
print(f" AUC Score               : {auc * 100:.2f}%")
print("\n Classification Report:\n", classification_report(y_test, y_pred))

# -------------------- Confusion Matrix --------------------
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# -------------------- ROC Curve --------------------
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Using XGBoost Boosting classification"""

np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=160, size=n_nonfraud),
    'Duration': np.random.normal(loc=11, scale=2.8, size=n_nonfraud),
    'IP_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.25, 0.75]),
    'Location_Match': np.random.choice([0, 1], size=n_nonfraud, p=[0.35, 0.65]),
    'Login_Attempts': np.random.poisson(2.0, size=n_nonfraud),
    'IsFraud': 0
})

fraud = pd.DataFrame({
    'Amount': np.random.exponential(scale=330, size=n_fraud),
    'Duration': np.random.normal(loc=19, scale=3.5, size=n_fraud),
    'IP_Match': np.random.choice([0, 1], size=n_fraud, p=[0.65, 0.35]),
    'Location_Match': np.random.choice([0, 1], size=n_fraud, p=[0.7, 0.3]),
    'Login_Attempts': np.random.poisson(4.2, size=n_fraud),
    'IsFraud': 1
})

df = pd.concat([nonfraud, fraud], ignore_index=True)

#----------------------------Train-Test Split---------------------------------
X = df.drop('IsFraud', axis=1)
y = df['IsFraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

#----------------------------Feature Scaling-----------------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#---------------------------- Apply SMOTE--------------------------------------
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

#---------------------------- Train XGBoost------------------------------------
xgb_model = XGBClassifier(
    n_estimators=300,
    learning_rate=0.08,
    max_depth=6,
    subsample=0.9,
    colsample_bytree=0.85,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)
xgb_model.fit(X_train_resampled, y_train_resampled)

#-----------------------Predict probabilities---------------------------
y_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]

#-----------------------Optimize threshold-------------------------------
best_threshold = 0.5
best_f1 = 0
for t in np.arange(0.1, 0.9, 0.01):
    y_temp = (y_proba >= t).astype(int)
    f1 = f1_score(y_test, y_temp)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

#------------------------Final prediction---------------------------------
y_pred = (y_proba >= best_threshold).astype(int)

#------------------------Evaluation----------------------------------------
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print(f" Best Threshold   : {round(best_threshold, 2)}")
print(f" Accuracy         : {accuracy * 100:.2f}%")
print(f" Precision        : {precision * 100:.2f}%")
print(f" Recall           : {recall * 100:.2f}%")
print(f" F1 Score         : {f1 * 100:.2f}%")
print(f" AUC Score        : {auc * 100:.2f}%")
print("\n Classification Report:\n", classification_report(y_test, y_pred))

#-------------------------Confusion Matrix---------------------------------
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Fraud', 'Fraud'],
            yticklabels=['Non-Fraud', 'Fraud'])
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

#------------------------- ROC Curve----------------------------------------
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}", color='darkgreen')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - XGBoost")
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()

"""Anomaly Detection or Resolving reconstruction error and imbalance using Deep Learning Model(Isolation Forest)"""

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix, roc_curve, auc as sk_auc
)
# -------------------- 1. Data Generation --------------------
np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({
    'TransactionAmount': np.random.exponential(scale=180, size=n_nonfraud),
    'TransactionType': np.random.choice(['Purchase', 'Withdrawal'], n_nonfraud),
    'Location': np.random.choice(['CityA', 'CityB'], n_nonfraud),
    'Channel': np.random.choice(['Mobile', 'ATM'], n_nonfraud),
    'CustomerAge': np.random.randint(35, 60, n_nonfraud),
    'TransactionDuration': np.random.normal(loc=7, scale=2.0, size=n_nonfraud),
    'LoginAttempts': np.random.poisson(2, size=n_nonfraud),
    'AccountBalance': np.random.normal(12000, 1200, size=n_nonfraud),
    'IsFraud': 0
})

fraud = pd.DataFrame({
    'TransactionAmount': np.random.exponential(scale=700, size=n_fraud),
    'TransactionType': np.random.choice(['Transfer', 'Purchase'], size=n_fraud, p=[0.8, 0.2]),
    'Location': np.random.choice(['CityC', 'CityB'], size=n_fraud, p=[0.8, 0.2]),
    'Channel': np.random.choice(['Web', 'Mobile'], size=n_fraud, p=[0.85, 0.15]),
    'CustomerAge': np.random.randint(25, 45, size=n_fraud),
    'TransactionDuration': np.random.normal(loc=13, scale=3.5, size=n_fraud),
    'LoginAttempts': np.random.poisson(4, size=n_fraud),
    'AccountBalance': np.random.normal(5000, 300, size=n_fraud),
    'IsFraud': 1
})

df = pd.concat([nonfraud, fraud], ignore_index=True)

# -------------------- 2. Preprocessing --------------------
X = df.drop("IsFraud", axis=1)
y = df["IsFraud"]

num_cols = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()

preprocessor = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_cols)
])

X_transformed = preprocessor.fit_transform(X)

# -------------------- 3. Train-Test Setup --------------------
X_normal = X_transformed[y == 0]
X_fraud = X_transformed[y == 1]

X_train_iso = X_normal  # Isolation Forest is trained only on normal samples
X_test = np.vstack([X_normal, X_fraud])
y_test = np.hstack([np.zeros(len(X_normal)), np.ones(len(X_fraud))])

# -------------------- 4. Train Isolation Forest --------------------
iso_model = IsolationForest(
    n_estimators=300,
    contamination=fraud_ratio,
    random_state=42
)
iso_model.fit(X_train_iso)

# -------------------- 5. Anomaly Score & Threshold Tuning --------------------
scores = -iso_model.decision_function(X_test)  # Higher = more anomalous

best_thresh = 0
best_f1 = 0
for t in np.linspace(min(scores), max(scores), 200):
    preds = (scores >= t).astype(int)
    f1 = f1_score(y_test, preds)
    if f1 > best_f1:
        best_f1 = f1
        best_thresh = t

y_pred_iso = (scores >= best_thresh).astype(int)

# -------------------- 6. Evaluation --------------------
print(f"\n--- Isolation Forest (Tuned) ---")
print(f"Best Threshold  : {best_thresh:.5f}")
print(f"Accuracy        : {accuracy_score(y_test, y_pred_iso) * 100:.2f}%")
print(f"Precision       : {precision_score(y_test, y_pred_iso) * 100:.2f}%")
print(f"Recall          : {recall_score(y_test, y_pred_iso) * 100:.2f}%")
print(f"F1 Score        : {f1_score(y_test, y_pred_iso) * 100:.2f}%")
print(f"AUC Score       : {roc_auc_score(y_test, scores) * 100:.2f}%")

# -------------------- 7. Confusion Matrix --------------------
cm = confusion_matrix(y_test, y_pred_iso)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='OrRd',
            xticklabels=["Non-Fraud", "Fraud"], yticklabels=["Non-Fraud", "Fraud"])
plt.title("Confusion Matrix - Isolation Forest (Tuned)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# -------------------- 8. ROC Curve --------------------
fpr, tpr, _ = roc_curve(y_test, scores)
roc_auc = sk_auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkred', lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Isolation Forest")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Anomaly Detection or Resolving reconstruction error and imbalance using Deep Learning Model(Auto Encoder)"""

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix, roc_curve, auc as sk_auc
)
# -------------------- 1. Data Generation --------------------
np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({
    'TransactionAmount': np.random.exponential(scale=150, size=n_nonfraud),
    'TransactionType': np.random.choice(['Purchase', 'Withdrawal'], n_nonfraud),
    'Location': np.random.choice(['CityA', 'CityB'], n_nonfraud),
    'Channel': np.random.choice(['Mobile', 'ATM'], n_nonfraud),
    'CustomerAge': np.random.randint(40, 65, n_nonfraud),
    'TransactionDuration': np.random.normal(loc=6, scale=1.0, size=n_nonfraud),
    'LoginAttempts': np.random.poisson(1, size=n_nonfraud),
    'AccountBalance': np.random.normal(15000, 1000, size=n_nonfraud),
    'IsFraud': 0
})

fraud = pd.DataFrame({
    'TransactionAmount': np.random.exponential(scale=1100, size=n_fraud),
    'TransactionType': ['Transfer'] * n_fraud,
    'Location': ['CityC'] * n_fraud,
    'Channel': ['Web'] * n_fraud,
    'CustomerAge': np.random.randint(18, 25, n_fraud),
    'TransactionDuration': np.random.normal(loc=20, scale=1.5, size=n_fraud),
    'LoginAttempts': np.random.poisson(8, size=n_fraud),
    'AccountBalance': np.random.normal(1000, 500, size=n_fraud),
    'IsFraud': 1
})

df = pd.concat([nonfraud, fraud], ignore_index=True)

# -------------------- 2. Preprocessing --------------------
X = df.drop("IsFraud", axis=1)
y = df["IsFraud"]

num_cols = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()

preprocessor = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_cols)
])

X_transformed = preprocessor.fit_transform(X)

# -------------------- 3. Train-Test Split --------------------
X_normal = X_transformed[y == 0]
X_fraud = X_transformed[y == 1]
X_train_norm, X_val_norm = train_test_split(X_normal, test_size=0.2, random_state=42)

X_test = np.vstack([X_val_norm, X_fraud])
y_test = np.hstack([np.zeros(len(X_val_norm)), np.ones(len(X_fraud))])

# -------------------- 4. Build Autoencoder --------------------
input_dim = X_train_norm.shape[1]
input_layer = Input(shape=(input_dim,))
x = Dense(64, activation='relu')(input_layer)
x = Dense(32, activation='relu')(x)
encoded = Dense(16, activation='relu')(x)
x = Dense(32, activation='relu')(encoded)
x = Dense(64, activation='relu')(x)
decoded = Dense(input_dim, activation='linear')(x)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# -------------------- 5. Train Autoencoder --------------------
autoencoder.fit(
    X_train_norm, X_train_norm,
    epochs=100,
    batch_size=32,
    validation_data=(X_val_norm, X_val_norm),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],
    verbose=0
)

# -------------------- 6. Predict & Threshold --------------------
mse_train = np.mean(np.square(X_train_norm - autoencoder.predict(X_train_norm)), axis=1)
mse_test = np.mean(np.square(X_test - autoencoder.predict(X_test)), axis=1)

threshold = np.percentile(mse_train, 99.9)
y_pred = (mse_test >= threshold).astype(int)

# -------------------- 7. Evaluation --------------------
print(f" Threshold Used : {threshold:.5f}")
print(f" Accuracy       : {accuracy_score(y_test, y_pred) * 100:.2f}%")
print(f" Precision      : {precision_score(y_test, y_pred) * 100:.2f}%")
print(f" Recall         : {recall_score(y_test, y_pred) * 100:.2f}%")
print(f" F1 Score       : {f1_score(y_test, y_pred) * 100:.2f}%")
print(f" AUC Score      : {roc_auc_score(y_test, mse_test) * 100:.2f}%")

# -------------------- 8. Confusion Matrix --------------------
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["Non-Fraud", "Fraud"], yticklabels=["Non-Fraud", "Fraud"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()

# -------------------- 9. ROC Curve --------------------
fpr, tpr, _ = roc_curve(y_test, mse_test)
roc_auc_value = sk_auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f"ROC Curve (AUC = {roc_auc_value:.2f})")
plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC Curve)")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Steganographic image detection to protect financial fraud"""

from PIL import Image

# ----------------- 1. Generate Binary Label Data  -----------------
np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({'IsFraud': [0]*n_nonfraud})
fraud = pd.DataFrame({'IsFraud': [1]*n_fraud})
df = pd.concat([nonfraud, fraud], ignore_index=True)

fraud_labels = df['IsFraud'].values.astype(np.uint8)

# ----------------- 2. RGB Steganography: Encode -----------------
def encode_labels_rgb(labels, output_path="fraud_rgb_stego.png"):
    padded_len = int(np.ceil(len(labels) / 3.0)) * 3
    padded_labels = np.pad(labels, (0, padded_len - len(labels)), 'constant')

    # Group into (R, G, B) LSB triplets
    pixels = np.random.randint(0, 256, (padded_len // 3, 3), dtype=np.uint8)
    pixels &= 0b11111110
    pixels |= padded_labels.reshape(-1, 3)

    # Make into square image
    size = int(np.ceil(np.sqrt(len(pixels))))
    pad_pixels = np.vstack([pixels, np.random.randint(0, 255, (size**2 - len(pixels), 3))])
    image_array = pad_pixels.reshape((size, size, 3))

    image = Image.fromarray(image_array, 'RGB')
    image.save(output_path)
    print(f"[✅] RGB Stego Image saved as: {output_path}")

# ----------------- 3. Decode From RGB Image -----------------
def decode_labels_rgb(image_path, original_length):
    image = Image.open(image_path).convert('RGB')
    image_array = np.array(image).reshape(-1, 3)
    bits = image_array[:, :3] & 1
    decoded_labels = bits.flatten()[:original_length]
    return decoded_labels

# ----------------- 4. Run and Verify -----------------
encode_labels_rgb(fraud_labels, "fraud_rgb_stego.png")
decoded_labels = decode_labels_rgb("fraud_rgb_stego.png", len(fraud_labels))

# ----------------- 5. Show Visual Stego Image -----------------
img = Image.open("fraud_rgb_stego.png")
plt.figure(figsize=(6, 6))
plt.imshow(img)
plt.title("RGB Steganographic Fraud Image")
plt.axis('off')
plt.tight_layout()
plt.show()

# ----------------- 6. Evaluate -----------------
acc = accuracy_score(fraud_labels, decoded_labels)
print(f"[📊] Recovered Label Accuracy from RGB Image: {acc * 100:.2f}%")

"""Grayscale image based Steganography"""

# ------------------ 1. Label Data ------------------
np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({'IsFraud': [0] * n_nonfraud})
fraud = pd.DataFrame({'IsFraud': [1] * n_fraud})
df = pd.concat([nonfraud, fraud], ignore_index=True)
fraud_labels = df['IsFraud'].values.astype(np.uint8)

# ------------------ 2. Encode as Grayscale Pixels ------------------
def encode_grayscale_stego(labels, output_path="fraud_grayscale_stego.png"):
    pixel_values = (labels * 255).astype(np.uint8)
    size = int(np.ceil(np.sqrt(len(pixel_values))))
    padded = np.pad(pixel_values, (0, size**2 - len(pixel_values)), constant_values=0)
    image_array = padded.reshape((size, size))

    image = Image.fromarray(image_array, mode='L')
    image.save(output_path)
    print(f"[✅] Grayscale Stego Image saved as: {output_path}")

# ------------------ 3. Decode from Grayscale ------------------
def decode_grayscale_stego(image_path, original_length):
    image = Image.open(image_path).convert('L')
    pixels = np.array(image).flatten()
    decoded = (pixels > 127).astype(np.uint8)
    return decoded[:original_length]

# ------------------ 4. Save & Verify ------------------
encode_grayscale_stego(fraud_labels, "fraud_grayscale_stego.png")
decoded_labels = decode_grayscale_stego("fraud_grayscale_stego.png", len(fraud_labels))

# ------------------ 5. Visualize ------------------
img = Image.open("fraud_grayscale_stego.png")
plt.figure(figsize=(6, 6))
plt.imshow(img, cmap='gray')
plt.title("Grayscale Steganographic Fraud Image")
plt.axis('off')
plt.tight_layout()
plt.show()

# ------------------ 6. Accuracy ------------------
acc = accuracy_score(fraud_labels, decoded_labels)
print(f"[📊] Recovered Label Accuracy from Grayscale Image: {acc * 100:.2f}%")

"""QRCode embedding based Steganography"""

import qrcode
from PIL import Image
import cv2

# ------------------ Step 1: Generate Fraud Labels ------------------
np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud

nonfraud = pd.DataFrame({'IsFraud': [0]*n_nonfraud})
fraud = pd.DataFrame({'IsFraud': [1]*n_fraud})
df = pd.concat([nonfraud, fraud], ignore_index=True)
fraud_labels = df['IsFraud'].astype(str).values

# ------------------ Step 2: Convert to Binary & Hex ------------------
binary_str = ''.join(fraud_labels)
hex_str = hex(int(binary_str, 2))[2:]

# ------------------ Step 3: Generate QR Code ------------------
qr = qrcode.QRCode(version=None, box_size=10, border=4)
qr.add_data(hex_str)
qr.make(fit=True)
qr_img = qr.make_image(fill_color="black", back_color="white").convert("RGB")
qr_img.save("qr_code.png")
qr_array = np.array(qr_img)

# ------------------ Step 4: Create Cover Image ------------------
h, w, _ = qr_array.shape
cover_image = np.random.randint(0, 256, (h + 20, w + 20, 3), dtype=np.uint8)
x_offset, y_offset = 10, 10

# ------------------ Step 5: Embed QR in Cover Image Using LSB ------------------
for i in range(h):
    for j in range(w):
        for k in range(3):  # RGB channels
            cover_image[y_offset + i, x_offset + j, k] &= 0b11111110
            cover_image[y_offset + i, x_offset + j, k] |= qr_array[i, j, k] & 0b00000001

stego_img = Image.fromarray(cover_image)
stego_img.save("hybrid_stego_qr_lsb.png")

# ------------------ Step 6: Extract QR Code from LSB ------------------
extracted_qr = np.zeros((h, w, 3), dtype=np.uint8)
for i in range(h):
    for j in range(w):
        for k in range(3):
            bit = cover_image[y_offset + i, x_offset + j, k] & 1
            extracted_qr[i, j, k] = 255 if bit else 0

extracted_img = Image.fromarray(extracted_qr)
extracted_img.save("extracted_qr.png")

# ------------------ Step 7: Decode QR ------------------
qr_detector = cv2.QRCodeDetector()
data, _, _ = qr_detector.detectAndDecode(cv2.imread("extracted_qr.png"))

# ------------------ Step 8: Recover Fraud Labels ------------------
if data:
    recovered_binary = bin(int(data, 16))[2:].zfill(len(fraud_labels))
    decoded_labels = np.array([int(b) for b in recovered_binary[:len(fraud_labels)]])
    accuracy = accuracy_score(df['IsFraud'].values, decoded_labels)
    print(f"[✅] Recovered Accuracy: {accuracy * 100:.2f}%")
else:
    print("[❌] QR decoding failed")

# ------------------ Step 9: Show Images ------------------
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.imshow(qr_img)
plt.title("Original QR Code")
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(stego_img)
plt.title("Stego Image (with QR LSB)")
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(extracted_img)
plt.title("Extracted QR from Stego")
plt.axis('off')

plt.tight_layout()
plt.show()

"""Histogram Modulation based Steganography"""

# -------------------- Step 1: Generate Fraud Labels --------------------
np.random.seed(42)
n = 1000
fraud_ratio = 0.07
n_fraud = int(n * fraud_ratio)
n_nonfraud = n - n_fraud
fraud_labels = np.array([0]*n_nonfraud + [1]*n_fraud)

# -------------------- Step 2: Create Grayscale Cover Image --------------------
cover_img = np.random.randint(0, 256, size=(64, 64), dtype=np.uint8)
cover_img_flat = cover_img.flatten()

# -------------------- Step 3: Embed Using Histogram Modulation --------------------
def embed_histogram_modulation(img_flat, labels):
    hist, _ = np.histogram(img_flat, bins=256, range=(0, 256))
    peak_bin = np.argmax(hist)
    zero_bin = (peak_bin + 1) % 256

    peak_indices = np.where(img_flat == peak_bin)[0]
    max_capacity = len(peak_indices)
    actual_labels = labels[:max_capacity]

    embedded = img_flat.copy()

    for i in range(len(actual_labels)):
        idx = peak_indices[i]
        if actual_labels[i] == 1:
            embedded[idx] = zero_bin

    return embedded, peak_bin, zero_bin, len(actual_labels), actual_labels

stego_flat, peak, zero, embedded_count, used_labels = embed_histogram_modulation(cover_img_flat, fraud_labels)
stego_img = stego_flat.reshape(cover_img.shape)
Image.fromarray(stego_img).save("histogram_stego.png")
print(f"[✅] Histogram-modulated stego image saved as 'histogram_stego.png'")
print(f"[ℹ️] Number of labels embedded: {embedded_count} / {len(fraud_labels)}")

# -------------------- Step 4: Extraction Function --------------------
def extract_from_histogram(stego_flat, peak_bin, zero_bin, count):
    extracted = []
    for val in stego_flat:
        if val == peak_bin:
            extracted.append(0)
        elif val == zero_bin:
            extracted.append(1)
        if len(extracted) == count:
            break
    return np.array(extracted)

# -------------------- Step 5: Extract and Evaluate --------------------
recovered_labels = extract_from_histogram(stego_flat, peak, zero, embedded_count)
accuracy = accuracy_score(used_labels, recovered_labels)
print(f"[📊] Recovery Accuracy (on embedded part): {accuracy * 100:.2f}%")

# -------------------- Step 6: Visualization --------------------
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.hist(cover_img_flat, bins=256, color='blue', alpha=0.6, label='Original')
plt.axvline(peak, color='blue', linestyle='--', label=f"Peak = {peak}")
plt.title("Original Histogram")
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(stego_flat, bins=256, color='green', alpha=0.6, label='Stego')
plt.axvline(peak, color='blue', linestyle='--', label=f"Peak = {peak}")
plt.axvline(zero, color='red', linestyle='--', label=f"Zero = {zero}")
plt.title("Stego Histogram (after embedding)")
plt.legend()

plt.tight_layout()
plt.show()