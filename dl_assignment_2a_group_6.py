# -*- coding: utf-8 -*-
"""DL_assignment_2A_group_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jwxCavpytDsC5pWnEv86hEXcBhLE_WBq

## Group No : 6

## Group Member Names:
1. Souvik Nag
2. Sudeet Mhatre
3. Soumen Upadhay
4. Apoorva Belsare

# 1. Import the required libraries
"""

##---------Type the code below this line------------------##
!pip install tensorflow_datasets

import tensorflow_datasets as tfds
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
import time
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.regularizers import l2
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping

# Download NLTK stopwords
nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('wordnet')

"""# 2. Data Acquisition  -- Score: 0.5 Mark

For the problem identified by you, students have to find the data source themselves from any data source.

## 2.1 Code for converting the above downloaded data into a form suitable for DL


"""

##---------Type the code below this line------------------##
# Load the IMDB reviews dataset
dataset, info = tfds.load("imdb_reviews", with_info=True, as_supervised=True)
dataset,info

"""## 2.1 Write your observations from the above.

1. Size of the dataset
2. What type of data attributes are there?
3. What are you classifying?
4. Plot the distribution of the categories of the target / label.


"""

# Observations on dataset size and data attributes
training_data = dataset['train']
testing_data = dataset['test']

# Dataset size
training_data_size = len(list(training_data))
testing_data_size = len(list(testing_data))
training_data_size, testing_data_size

# Data attributes
dataset_attributes = info.features
dataset_attributes

# What are we classifying? (Sentiment: positive or negative)
classification_target = dataset_attributes['label']
classification_target

# Plotting distribution of categories (positive and negative)
dataset_labels = [label.numpy() for _, label in training_data]
plt.figure(figsize=(8, 6))
plt.hist(dataset_labels, bins=2, edgecolor='black')
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Distribution of Sentiment Labels in IMDb Reviews')
plt.show()

"""# 3. Data Preparation -- Score: 1 Mark

Perform the data prepracessing that is required for the data that you have downloaded.


This stage depends on the dataset that is used.

## 3.1 Apply pre-processing techiniques

* to remove duplicate data
* to impute or remove missing data
* to remove data inconsistencies
* Encode categorical data
* Normalize the data
* Feature Engineering
* Stop word removal, lemmatiation, stemming, vectorization


IF ANY
"""

##---------Type the code below this line------------------##

# Convert TensorFlow dataset to Pandas DataFrame
training_data = [(text.numpy().decode('utf-8'), label.numpy()) for text, label in dataset['train']]
testing_data = [(text.numpy().decode('utf-8'), label.numpy()) for text, label in dataset['test']]

training_df = pd.DataFrame(training_data, columns=['review', 'label'])
testing_df = pd.DataFrame(testing_data, columns=['review', 'label'])

# 1. Remove Duplicates from training dataset
print(f" Before removing duplicates, the data count is: ",training_df.count())
training_df.drop_duplicates(subset=['review'], inplace=True)
print(f" After removing duplicates, the data count is: ",training_df.count())

# 2. Handle Missing Data in training and testing dataframes
training_df.dropna(inplace=True)
testing_df.dropna(inplace=True)

# 3. Remove Data Inconsistencies (e.g., extra spaces, special characters)
def transform_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    return text.strip()

training_df['review'] = training_df['review'].apply(transform_text)
testing_df['review'] = testing_df['review'].apply(transform_text)
print(f"After removing data inconsistencies, training data looks like:")
training_df.head()


# 4. Encode Categorical Data (Label Encoding)
label_encoder = LabelEncoder()
training_df['label'] = label_encoder.fit_transform(training_df['label'])
testing_df['label'] = label_encoder.transform(testing_df['label'])

# 5. Stop Word Removal, Lemmatization and Stemming
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

def normalise_text(text):
    words = word_tokenize(text)  # Tokenize
    words = [word for word in words if word not in stop_words]  # Remove stop words
    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization
    words = [stemmer.stem(word) for word in words]  # Stemming
    return ' '.join(words)

training_df['cleaned_review'] = training_df['review'].apply(normalise_text)
testing_df['cleaned_review'] = testing_df['review'].apply(normalise_text)

# 6. Vectorization (TF-IDF)
vectorizer = TfidfVectorizer(max_features=5000)
X_train = vectorizer.fit_transform(training_df['cleaned_review']).toarray()
X_test = vectorizer.transform(testing_df['cleaned_review']).toarray()

y_train = training_df['label'].values
y_test = testing_df['label'].values

# 7. Plot Distribution of Labels
sns.countplot(x=training_df['label'])
plt.xticks([0, 1], ['Negative', 'Positive'])
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.title("Distribution of IMDb Sentiment Labels")
plt.show()

# Print final dataset shape
print(f"Shape of the training data is:: {X_train.shape}, and shape of the testing data is:: {X_test.shape}")

"""## 3.2 Identify the target variables.

* Separate the data front the target such that the dataset is in the form of (X,y) or (Features, Label)

* Discretize / Encode the target variable or perform one-hot encoding on the target or any other as and if required.




"""

##---------Type the code below this line------------------##
# 1. Identify the target variable (Sentiment Label: 0 = Negative, 1 = Positive)
target_variable = 'label'

# 2. Separate Features (X) and Target (y)
X = X_train
y = y_train

# 3. Encode the Target Variable (If required)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # Ensure labels are numeric (0 or 1)
label_encoder,y

"""## 3.3 Split the data into training set and testing set"""

##---------Type the code below this line------------------##
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print dataset shapes
print(f"Training Data: X_train = {X_train.shape}, y_train = {y_train.shape}")
print(f"Testing Data: X_test = {X_test.shape}, y_test = {y_test.shape}")

"""## 3.4 Preprocessing report

Mention the method adopted  and justify why the method was used
* to remove duplicate data, if present
* to impute or remove missing data, if present
* to remove data inconsistencies, if present
* to encode categorical data
* the normalization technique used

If the any of the above are not present, then also add in the report below.

Report the size of the training dataset and testing dataset

"""

training_df = pd.DataFrame(training_data, columns=['review', 'label'])
testing_df = pd.DataFrame(testing_data, columns=['review', 'label'])

# Data Preprocessing
training_df.drop_duplicates(subset=['review'], inplace=True)
training_df.dropna(inplace=True)
testing_df.dropna(inplace=True)

# Text Cleaning
def transform_text(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return text.strip()

training_df['review'] = training_df['review'].apply(transform_text)
testing_df['review'] = testing_df['review'].apply(transform_text)

# Label Encoding
label_encoder = LabelEncoder()
training_df['label'] = label_encoder.fit_transform(training_df['label'])
testing_df['label'] = label_encoder.transform(testing_df['label'])

# Tokenization and Stemming
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

def normalise_text(text):
    words = word_tokenize(text)
    words = [word for word in words if word not in stop_words]
    words = [lemmatizer.lemmatize(word) for word in words]
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

training_df['cleaned_review'] = training_df['review'].apply(normalise_text)
testing_df['cleaned_review'] = testing_df['review'].apply(normalise_text)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train = vectorizer.fit_transform(training_df['cleaned_review']).toarray()
X_test = vectorizer.transform(testing_df['cleaned_review']).toarray()

y_train = training_df['label'].values
y_test = testing_df['label'].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

"""Preprocessing Report

1. Data Cleaning
Removed duplicate reviews from training_df.
Dropped missing values from training_df and testing_df.
Converted text to lowercase, removed extra spaces, and non-alphanumeric characters.

2. Label Encoding & Normalization
Encoded labels using LabelEncoder.
Tokenized text, removed stopwords, applied lemmatization & stemming.

3. Vectorization & Splitting
Used TfidfVectorizer (5000 features) for text transformation.
Split data (80% train, 20% test) using train_test_split.

4. Summary
Training Samples: X_train.shape[0]
Testing Samples: X_test.shape[0]
Vocabulary Size: 5000

# 4. Deep Neural Network Architecture - Score:  Marks

## 4.1 Design the architecture that you will be using

* Sequential Model Building with Activation for each layer.
* Add dense layers, specifying the number of units in each layer and the activation function used in the layer.
* Use Relu Activation function in each hidden layer
* Use Sigmoid / softmax Activation function in the output layer as required

DO NOT USE CNN OR RNN.
"""

##---------Type the code below this line------------------##
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Define the Neural Network Architecture
model = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),  # Input Layer
    Dropout(0.3),  # Dropout for Regularization

    Dense(256, activation='relu'),  # Hidden Layer 1
    Dropout(0.3),

    Dense(128, activation='relu'),  # Hidden Layer 2
    Dropout(0.3),

    Dense(64, activation='relu'),  # Hidden Layer 3
    Dropout(0.3),

    Dense(1, activation='sigmoid')  # Output Layer for Binary Classification
])

# Compile the Model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Display the Model Summary
model.summary()

# Display the Model Summary
model.summary()

"""## 4.2 DNN Report

Report the following and provide justification for the same.



* Number of layers
* Number of units in each layer
* Total number of trainable parameters


"""

##---------Type the answer below this line------------------##

Why These Layers?

More hidden layers → Enhance the ability to identify intricate patterns in textual data.
ReLU activation in hidden layers → Mitigates vanishing gradients and accelerates learning.
Sigmoid activation in output layer → Produces probabilities for binary classification.

Why These Units?

512 neurons in the input layer → Sufficient to encapsulate high-dimensional TF-IDF features.
Gradual reduction (256 → 128 → 64 neurons) → Decreases complexity and minimizes overfitting.

Why Dropout Layers?

Dropout (0.3) curbs overfitting by randomly disabling neurons during training.

"""# 5. Training the model - Score: 1 Mark

## 5.1 Configure the training

Configure  the model for training, by using appropriate optimizers and regularizations

Compile with categorical CE loss and metric accuracy.
"""

##---------Type the code below this line------------------##
# Compile the Model
optimizer = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

"""## 5.2 Train the model

Train Model with cross validation, with total time taken shown for 20 epochs.

Use SGD.
"""

##---------Type the code below this line------------------##
# Define K-Fold Cross Validation
k = 5  # Number of folds
kfold = KFold(n_splits=k, shuffle=True, random_state=42)

# Store results
accuracies_of_kf = []
start_time = time.time()  # Track total training time

for train_index, val_index in kfold.split(X_train):
    # Split into training and validation sets
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Define the Model
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer for binary classification
    ])

    # Compile Model using SGD Optimizer
    model.compile(
        optimizer=SGD(learning_rate=0.01, momentum=0.9),  # SGD with momentum
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    # Train Model with Early Stopping
    history = model.fit(
        X_train_fold, y_train_fold,
        validation_data=(X_val_fold, y_val_fold),
        epochs=20,
        batch_size=32,
        verbose=1
    )

    # Store final validation accuracy
    final_accuracy = history.history['val_accuracy'][-1]
    accuracies_of_kf.append(final_accuracy)

end_time = time.time()  # End timing

# Print Cross-Validation Results
print("\nCross-Validation Accuracy per Fold is: ", accuracies_of_kf)
print("Average Accuracy of the model with KFold is: ", np.mean(accuracies_of_kf))
print(f"Total Training Time for 20 Epochs is: {end_time - start_time:.2f} seconds")

"""Justify your choice of optimizers and regulizations used and the hyperparameters tuned

"""

##---------Type the answers below this line------------------##
Optimizers:

Adam:
This adapts learning rates for each parameter, handles sparse gradients well.
It has a learning rate of 0.0001 which is  better for convergence.
SGD with Momentum:
It helps escaping local minima by smoothing updates.
It has learning rate of 0.01 and momentum 0.9 which are effective for efficient convergence without overshooting.

Regularizations:

L2 Regularization (0.001) prevents overfitting by penalizing large weights, encouraging a simpler model.
Dropout randomly drops neurons to ensure robustness, reducing dependency on any single feature.

Hyperparameters:

Layer Architecture (512, 256, 128, 64) helps in progressive reduction in neurons captures high-level features and simplifies representations.
Batch Size (32) balances gradient noise and computational efficiency.
Epochs (20) are sufficient to learn without overfitting; cross-validation further validates performance.
5-Fold Cross-Validation provides a robust performance estimate across different data splits.

"""# 6. Test the model - 0.5 marks

"""

##---------Type the code below this line------------------##
# Evaluate the Model on the Test Set
loss_on_test_kf, accuracy_on_test_kf = model.evaluate(X_test, y_test, verbose=1)

# Print Final Test Accuracy & Loss
print(f"\nModel Test Accuracy is: {accuracy_on_test_kf:.4f}")
print(f"\nModel Test Loss is: {loss_on_test_kf:.4f}")

"""# 7. Intermediate result  - Score: 1 mark

1. Plot the training and validation accuracy history.
2. Plot the training and validation loss history.
3. Report the testing accuracy and loss.
4. Show Confusion Matrix for testing dataset.
5. Report values for preformance study metrics like accuracy, precision, recall, F1 Score.

"""

##---------Type the code below this line------------------##
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# 1. Plot the training and validation accuracy history.
plt.figure(figsize=(8, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()
plt.show()

# 2. Plot the training and validation loss history.
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()
plt.show()

# 3. Report the testing accuracy and loss.
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# 4. Show Confusion Matrix for testing dataset.
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary labels

cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# 5. Report performance metrics: accuracy, precision, recall, F1 Score.
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

"""# 8. Model architecture - Score: 1 mark


Modify the architecture designed in section 4.1

1. by decreasing one layer
2. by increasing one layer

For example, if the architecture in 4.1 has 5 layers, then 8.1 should have 4 layers and 8.2 should have 6 layers.

Plot the comparison of the training and validation accuracy of the three architecures (4.1, 8.1 and 8.2)


"""

##---------Type the code below this line------------------##
def create_actual_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_reduced_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_expanded_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Train Original Model (5 Layers)
actual_model_with_5_layers = create_actual_model()
actual_history = actual_model_with_5_layers.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

# Train Reduced Model (4 Layers)
reduced_model_with_4_layers = create_reduced_model()
reduced_history_with_4_layers = reduced_model_with_4_layers.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

# Train Expanded Model (6 Layers)
expanded_model_with_6_layers = create_expanded_model()
expanded_history_with_6_layers = expanded_model_with_6_layers.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

import matplotlib.pyplot as plt

# Plot Training Accuracy
plt.plot(actual_history.history['accuracy'], label='Original (5 Layers)')
plt.plot(reduced_history_with_4_layers.history['accuracy'], label='Reduced (4 Layers)')
plt.plot(expanded_history_with_6_layers.history['accuracy'], label='Expanded (6 Layers)')

plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Comparison of Training Accuracy')
plt.legend()
plt.show()

# Plot Validation Accuracy
plt.plot(actual_history.history['val_accuracy'], label='Original (5 Layers)')
plt.plot(reduced_history_with_4_layers.history['val_accuracy'], label='Reduced (4 Layers)')
plt.plot(expanded_history_with_6_layers.history['val_accuracy'], label='Expanded (6 Layers)')

plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.title('Comparison of Validation Accuracy')
plt.legend()
plt.show()

"""# 9. Regularisations - Score: 1 mark

Modify the architecture designed in section 4.1

1. Dropout of ratio 0.25
2. Dropout of ratio 0.25 with L2 regulariser with factor 1e−04.

Plot the comparison of the training and validation accuracy of the three (4.1, 9.1 and 9.2)


"""

##---------Type the code below this line------------------##
def create_actual_model_3_dropout():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_25_dropout_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.25),

        Dense(256, activation='relu'),
        Dropout(0.25),

        Dense(128, activation='relu'),
        Dropout(0.25),

        Dense(64, activation='relu'),
        Dropout(0.25),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_25_dropout_l2_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
        Dropout(0.25),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
        Dropout(0.25),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
        Dropout(0.25),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
        Dropout(0.25),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Train Original Model (4.1)
actual_model_3_dropout = create_actual_model_3_dropout()
actual_history_3_dropout = actual_model_3_dropout.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

# Train Model with Dropout 0.25 (9.1)
dropout_25_model = create_25_dropout_model()
dropout_25_history = dropout_25_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

# Train Model with Dropout 0.25 + L2 (9.2)
dropout_25_l2_model = create_25_dropout_l2_model()
dropout_25_l2_history = dropout_25_l2_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

import matplotlib.pyplot as plt

# Plot Training Accuracy
plt.plot(actual_history_3_dropout.history['accuracy'], label='Original (Dropout 0.3)')
plt.plot(dropout_25_history.history['accuracy'], label='Dropout 0.25')
plt.plot(dropout_25_l2_history.history['accuracy'], label='Dropout 0.25 + L2 (1e-4)')

plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Comparison of Training Accuracy')
plt.legend()
plt.show()

# Plot Validation Accuracy
plt.plot(actual_history_3_dropout.history['val_accuracy'], label='Original (Dropout 0.3)')
plt.plot(dropout_25_history.history['val_accuracy'], label='Dropout 0.25')
plt.plot(dropout_25_l2_history.history['val_accuracy'], label='Dropout 0.25 + L2 (1e-4)')

plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.title('Comparison of Validation Accuracy')
plt.legend()
plt.show()

"""# 10. Optimisers -Score: 1 mark

Modify the code written in section 5.2

1. RMSProp with your choice of hyper parameters
2. Adam with your choice of hyper parameters

Plot the comparison of the training and validation accuracy of the three (5.2, 10.1 and 10.2)

"""

##---------Type the code below this line------------------##
def create_sgd_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    optimizer = SGD(learning_rate=0.01, momentum=0.9)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_rmsprop_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    optimizer = RMSprop(learning_rate=0.001, rho=0.9)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_rmsprop_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    optimizer = RMSprop(learning_rate=0.001, rho=0.9)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_adam_model():
    model = tf.keras.Sequential([
        Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        Dropout(0.3),

        Dense(1, activation='sigmoid')  # Output Layer
    ])

    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Import required optimizers
from tensorflow.keras.optimizers import RMSprop, Adam, SGD

# Train Original Model (SGD - 5.2)
sgd_model = create_sgd_model()
sgd_history = sgd_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

# Train Model with RMSProp (10.1)
rmsprop_model = create_rmsprop_model()
rmsprop_history = rmsprop_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

# Train Model with Adam (10.2)
adam_model = create_adam_model()
adam_history = adam_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, verbose=1)

import matplotlib.pyplot as plt

# Plot Training Accuracy
plt.plot(sgd_history.history['accuracy'], label='SGD (5.2)')
plt.plot(rmsprop_history.history['accuracy'], label='RMSProp (10.1)')
plt.plot(adam_history.history['accuracy'], label='Adam (10.2)')

plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.title('Comparison of Training Accuracy (SGD vs RMSProp vs Adam)')
plt.legend()
plt.show()

# Plot Validation Accuracy
plt.plot(sgd_history.history['val_accuracy'], label='SGD (5.2)')
plt.plot(rmsprop_history.history['val_accuracy'], label='RMSProp (10.1)')
plt.plot(adam_history.history['val_accuracy'], label='Adam (10.2)')

plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.title('Comparison of Validation Accuracy (SGD vs RMSProp vs Adam)')
plt.legend()
plt.show()

"""# 11. Conclusion - Score: 1 mark

Comparing the sections 4.1, 5.2, 8, 9, and 10, present your observations on which model or architecture or regualiser or optimiser perfomed better.

"""

##---------Type the code below this line------------------##
print("### Observations and Inferences ###\n")

# Architecture Comparison
print("1. Architectural Comparison:")
print("   - Actual Model (5 Layers): Achieved a balanced performance with sufficient capacity and effective learning.")
print("   - Reduced Model (4 Layers): Lost some capacity, often underperforming compared to the original model.")
print("   - Expanded Model (6 Layers): Introduced extra complexity and a higher risk of overfitting without consistent benefits.\n")

# Regularization Comparison
print("2. Regularization Comparison:")
print("   - Models with Dropout at 0.3 provided regularization, but lowering dropout to 0.25 improved learning dynamics.")
print("   - Combining Dropout 0.25 with L2 Regularization (factor=1e-4) resulted in smoother training curves and better validation accuracy.")
print("   - This combination appears to be more effective in mitigating overfitting.\n")

# Optimizer Comparison
print("3. Optimizer Comparison:")
print("   - SGD with Momentum converged steadily but slower and sometimes to a lower final accuracy.")
print("   - RMSProp showed faster convergence than SGD but did not consistently match the accuracy levels achieved by Adam.")
print("   - Adam, with its adaptive learning rates, achieved the best overall performance, faster convergence, and higher accuracy.\n")

# Final Conclusion
print("### Final Conclusion:")
print("The best performance overall was achieved using the Original 5-layer architecture combined with effective regularization (Dropout 0.25 + L2) and optimized using the Adam optimizer.")

"""### NOTE


All Late Submissions will incur a <b>penalty of -2 marks </b>. So submit your assignments on time.

Good Luck
"""