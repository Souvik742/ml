# -*- coding: utf-8 -*-
"""ML_Group2_Assignment 8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQ0d6Cx5Wdbe85socWc5WmHfnRWg6QJb

## **ML Group 12**

**Group Member Names**:
1. SOUMEN UPADHYAY
2. SOUVIK NAG
3. BELSARE APOORVA UMESH
4. ATHIA RANA
5. MHATRE SUDEET ULHAS

**ASSIGNMENT DATASET**

diabetes_risk_prediction_dataset

https://drive.google.com/file/d/1fQhVx2on3WoMCu-R8PPCNvCAWdknTOAR/view?usp=drive_link

**ASSIGNMENT PART1**
"""

# Importing the pandas and numpy libraries, which is required for data manipulation and analysis
import numpy as np
import pandas as pd

#file path to the dataset
inp_file = '/content/diabetes_risk_prediction_dataset.csv'

#Reading the CSV file into a pandas DataFrame
df = pd.read_csv(inp_file)

#Print 2 rows for sanity check to identify all the features present in the dataset and if the target matches with them.
df.head(2)

import plotly.express as px

"""**Data Visualization and Exploration**"""

# Importing libraries necessary for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

"""**Display the column headings, statistical
information, description and statistical summary of
the data.**
"""

print("Column headings : ")
df.columns

print("Description :")
df.info()

print("Statistical information : ")
df.describe()

df.hist(bins=30, figsize=(10, 8))
plt.show()

# Pairplot for visualizing relationships
sns.pairplot(df)
plt.show()

import plotly.express as px

labels = ["Positive", "Negative"]

# Get pie chart slice values
values = df["class"].value_counts().to_numpy()
px.pie(values = values, names=labels, hole=0.4, title="Class Imbalance",width = 600)

"""**Identify null and missing**

"""

#Identify null and missing

# Check for missing values
print(df.isnull().sum())

# Fill or drop missing values
df.fillna(method='ffill', inplace=True)

"""**Check for duplicate rows**

"""

# Check for duplicate rows
duplicates = df.duplicated()
print(f"Number of duplicate rows: {duplicates.sum()}")

# Display duplicate rows if any
print("Duplicate rows:")
print(df[duplicates])

# Drop duplicate rows
df.drop_duplicates(inplace=True)

# Verify that duplicates have been removed
print(f"Number of duplicate rows after removal: {df.duplicated().sum()}")

"""**Encoding categorical data using Label Encoder**"""

#Importing the LabelEncoder class from the sklearn.preprocessing module
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

#Identify the categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print(categorical_cols)

#Encoding categorical variables using LabelEncoder
for col in categorical_cols:
    df[col] = encoder.fit_transform(df[col])

df.info()

"""**Correlation matrix generation**"""

# Correlation matrix
corr_matrix = df.corr()
print(corr_matrix)

"""**Heatmap for visualization**

The HeatMap was utilized in order to assess the relationship between the various features
and the target (satisfaction). Positive correlations are denoted by positive values, negative
correlations by negative values, and weak correlations by values close to zero on the
HeatMap.
"""

# Find highly correlated features (where correlation threshold is nearer to 1)
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

plt.title("Correlation Heatmap between Features and Target")
plt.show()

"""**Standardization on Age column**

Standardize the Age column in data, so that it will have a mean of 0 and a standard deviation of 1
"""

# Standardization
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Apply standardization only to the 'Age' column
df['Age'] = scaler.fit_transform(df[['Age']])

# Display the first few rows of the DataFrame to verify the change
print(df.head(2))

"""**Separate the data from the target such that the dataset is
(Features, Label)**
"""

# 'df' is DataFrame and 'class' is the target column
X = df.drop(columns=['class'])  # Features
y = df['class']  # label variable

"""**Split the dataset into training and testing sets in ratio of 80 : 20**

The main reason for splitting the dataset into training and test sets is to evaluate the modelâ€™s performance on unseen data. The training set is used to train the model, while the test set is used to simulate how the model will perform on new, unseen data
"""

from sklearn.model_selection import train_test_split

#test_size=0.2: 20% of the data will be used for testing, and the remaining 80% for training.

#Setting a random_state ensures that the data split is reproducible to maintain consistency
X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size (80%): {X_train_80.shape[0]} samples")
print(f"Test set size (20%): {X_test_20.shape[0]} samples")

"""**Logistic Regression implementation-80:20**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#The regularization parameter C=1.0 helps prevent overfitting, and setting random_state=42 ensures the results are consistent.
logistic_model = LogisticRegression(C=1.0, random_state=42)

# Train the model on the training data
logistic_model.fit(X_train_80, y_train_80)

# Predict on the test set
y_pred = logistic_model.predict(X_test_20)

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Logistic Regression Test Accuracy: {accuracy:.4f}")

"""**Calculation of evaluation parameters-80:20**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Define the model with a specific hyperparameter value for C
logistic_model = LogisticRegression(C=1.0, random_state=42)  # You can adjust C manually

# Train the model on the training data
logistic_model.fit(X_train_80, y_train_80)

# Predict on the test set
y_pred = logistic_model.predict(X_test_20)
y_pred_proba = logistic_model.predict_proba(X_test_20)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Logistic Regression Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_20, y_pred)
print(f"Logistic Regression Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_20, y_pred)
print(f"Logistic Regression Test Recall: {recall:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_20, y_pred_proba)
print(f"Logistic Regression Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_20, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"Logistic Regression Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_20, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Decision tree implementation-80:20**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Define the model with default hyperparameters or set specific ones
decision_tree_model = DecisionTreeClassifier(random_state=42)  # You can adjust parameters as needed

# Train the model on the training data
decision_tree_model.fit(X_train_80, y_train_80)

# Predict on the test set
y_pred = decision_tree_model.predict(X_test_20)

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Decision Tree Test Accuracy: {accuracy:.4f}")

"""**Evaluation parameters generation for decision tree-80:20**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Define the model with default hyperparameters or set specific ones
decision_tree_model = DecisionTreeClassifier(random_state=42)  # You can adjust parameters as needed

# Train the model on the training data
decision_tree_model.fit(X_train_80, y_train_80)

# Predict on the test set
y_pred = decision_tree_model.predict(X_test_20)
y_pred_proba = decision_tree_model.predict_proba(X_test_20)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Decision Tree Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_20, y_pred)
print(f"Decision Tree Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_20, y_pred)
print(f"Decision Tree Test Recall: {recall:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_20, y_pred_proba)
print(f"Decision Tree Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_20, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"Logistic Regression Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_20, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Performance Evaluation (80:20 split)**
1. The Decision Tree has slightly higher **accuracy** (0.8824) compared to Logistic Regression (0.8235).
2. **Precision** is higher for the Decision Tree (0.8919) than for Logistic Regression (0.8250).
3. Logistic Regression has a higher **recall** (0.9498) compared to the Decision Tree (0.9429).
4. Logistic Regression has a significantly higher **AUC-ROC** (0.9446) compared to the Decision Tree (0.8464). A higher AUC-ROC indicates that Logistic Regression has a better overall ability to distinguish between the positive and negative classes across all metrics.

**Comparison and Analysis (80:20)**

| Metric         | Decision Tree | Logistic Regression |
|----------------|---------------|---------------------|
| Accuracy       | 0.8824        | 0.8235              |
| Precision      | 0.8919        | 0.8250              |
| Recall         | 0.9429        | 0.9498              |
| AUC-ROC        | 0.8464        | 0.9446              |

**Splitting the data into train and test in the ratio of 70:30**
"""

# Example with 70% training and 30% testing
X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"Training set size (70%): {X_train_70.shape[0]} samples")
print(f"Test set size (30%): {X_test_30.shape[0]} samples")

"""**Logistic Regression implementation-(70:30)**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Define the model with a specific hyperparameter value for C
logistic_model = LogisticRegression(C=1.0, random_state=42)  # You can adjust C manually

# Train the model on the training data
logistic_model.fit(X_train_70, y_train_70)

# Predict on the test set
y_pred = logistic_model.predict(X_test_30)

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Logistic Regression Test Accuracy: {accuracy:.4f}")

"""**Evaluation parameters logistic Regression(70:30)**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Define the model with a specific hyperparameter value for C
logistic_model = LogisticRegression(C=1.0, random_state=42)  # You can adjust C manually

# Train the model on the training data
logistic_model.fit(X_train_70, y_train_70)

# Predict on the test set
y_pred = logistic_model.predict(X_test_30)
y_pred_proba = logistic_model.predict_proba(X_test_30)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Logistic Regression Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_30, y_pred)
print(f"Logistic Regression Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_30, y_pred)
print(f"Logistic Regression Test Recall: {recall:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_30, y_pred_proba)
print(f"Logistic Regression Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_30, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"Logistic Regression Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_30, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Decision Tree implementation(70:30)**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Define the model with default hyperparameters or set specific ones
decision_tree_model = DecisionTreeClassifier(random_state=42)  # You can adjust parameters as needed

# Train the model on the training data
decision_tree_model.fit(X_train_70, y_train_70)

# Predict on the test set
y_pred = decision_tree_model.predict(X_test_30)

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Decision Tree Test Accuracy: {accuracy:.4f}")

"""**Evaluation parameters Decision Tree(70:30)**



"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Define the model with default hyperparameters or set specific ones
decision_tree_model = DecisionTreeClassifier(random_state=42)  # You can adjust parameters as needed

# Train the model on the training data
decision_tree_model.fit(X_train_70, y_train_70)

# Predict on the test set
y_pred = decision_tree_model.predict(X_test_30)
y_pred_proba = decision_tree_model.predict_proba(X_test_30)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Decision Tree Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_30, y_pred)
print(f"Decision Tree Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_30, y_pred)
print(f"Decision Tree Test Recall: {recall:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_30, y_pred_proba)
print(f"Decision Tree Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_30, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")


# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_30, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Performance Evaluation (70:30 split)**
1. The Decision Tree has slightly higher **accuracy** (0.8816) compared to Logistic Regression (0.8421).
2. **Precision** is higher for the Decision Tree (0.8846) than for Logistic Regression (0.8245).
3. Logistic Regression has a higher **recall** (0.9592) compared to the Decision Tree (0.9388).
4. Logistic Regression has a significantly higher **AUC-ROC** (0.9562) compared to the Decision Tree (0.8583). A higher AUC-ROC indicates that Logistic Regression has a better overall ability to distinguish between the positive and negative classes across all metrics.

**Comparison and Analysis (70:30)**


| Metric         | Decision Tree | Logistic Regression |
|----------------|---------------|---------------------|
| Accuracy       | 0.8816        | 0.8421              |
| Precision      | 0.8846        | 0.8245              |
| Recall         | 0.9388        | 0.9592              |
| AUC-ROC        | 0.8583        | 0.9562              |

The differences in evaluation metrics between the 70:30 and 80:20 splits are minor.

1. In Diabetes Risk prediction generally, recall (sensitivity) and AUC-ROC are often prioritized as missing a Diabetes Risk (false negative) can be life-threatening.

2. Recall is slightly higher for both models in the 70:30 split, with Logistic Regression having the highest recall and AUC - ROC score

## **Assignment Part 2**:

**Model Building**

**Performance Evaluation**

**Model building using K-Nearest Neighbors Classifier(80:20)**
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Step 1: Initialize the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)

# Step 2: Train the model on the training data
knn_model.fit(X_train_80, y_train_80)

# Step 3: Predict on the test set
y_pred = knn_model.predict(X_test_20)

# Step 4: Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"KNN Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameters KNN classifier(80:20)**"""

from sklearn.metrics import  precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns

y_pred_proba = knn_model.predict_proba(X_test_20)[:, 1]  # Probability estimates for the positive class

#Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"KNN Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_20, y_pred)
print(f"KNN Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_20, y_pred)
print(f"KNN Test Recall: {recall:.4f}")

# Calculate F1-score
f1 = f1_score(y_test_20, y_pred)
print(f"KNN Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_20, y_pred_proba)
print(f"KNN Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_20, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"KNN Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_20, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Model Building using KNN Classifier(70:30)**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Step 1: Initialize the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)

# Step 2: Train the model on the training data
knn_model.fit(X_train_70, y_train_70)

# Step 3: Predict on the test set
y_pred = knn_model.predict(X_test_30)

# Step 4: Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"KNN Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameter-KNN Classifier(70:30)**"""

from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns

y_pred_proba = knn_model.predict_proba(X_test_30)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"KNN Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_30, y_pred)
print(f"KNN Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_30, y_pred)
print(f"KNN Test Recall: {recall:.4f}")


# Calculate F1 score
f1 = f1_score(y_test_30, y_pred)
print(f"KNN Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_30, y_pred_proba)
print(f"KNN Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_30, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"KNN Misclassification Rate: {misclassification_rate:.4f}")


# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_30, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Model Building using Naive Bayes Classifier(80:20)**"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Initialize the Naive Bayes model
naive_bayes_model = GaussianNB()

# Train the model on the training data
naive_bayes_model.fit(X_train_80, y_train_80)

# Predict on the test set
y_pred = naive_bayes_model.predict(X_test_20)

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Naive Bayes Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameter-Naive Bayes Classifier(80:20)**"""

from sklearn.metrics import  precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns


# Predict on the test set
y_pred = naive_bayes_model.predict(X_test_20)
y_pred_proba = naive_bayes_model.predict_proba(X_test_20)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Naive Bayes Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_20, y_pred)
print(f"Naive Bayes Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_20, y_pred)
print(f"Naive Bayes Test Recall: {recall:.4f}")

# Calculate F1 score
f1 = f1_score(y_test_20, y_pred)
print(f"Naive Bayes Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_20, y_pred_proba)
print(f"Naive Bayes Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_20, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"Naive Bayes Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_20, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Model Building using Naive Bayes Classifier(70:30)**"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Initialize the Naive Bayes model
naive_bayes_model = GaussianNB()

# Train the model on the training data
naive_bayes_model.fit(X_train_70, y_train_70)

# Predict on the test set
y_pred = naive_bayes_model.predict(X_test_30)

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Naive Bayes Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameter-Naive Bayes Classifier(70:30)**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns


y_pred_proba = naive_bayes_model.predict_proba(X_test_30)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Naive Bayes Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_30, y_pred)
print(f"Naive Bayes Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_30, y_pred)
print(f"Naive Bayes Test Recall: {recall:.4f}")

# Calculate F1 score
f1 = f1_score(y_test_30, y_pred)
print(f"Naive Bayes Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_30, y_pred_proba)
print(f"Naive Bayes Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_30, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"Naive Bayes Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_30, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Model Building Using random forest Classifier(80:20)**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Initialize the Random Forest model
random_forest_model = RandomForestClassifier()

# Train the model on the training data
random_forest_model.fit(X_train_80, y_train_80)

# Predict on the test set
y_pred = random_forest_model.predict(X_test_20)

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Random Forest Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameter-Random Forest Classifier(80:20)**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns

y_pred_proba = random_forest_model.predict_proba(X_test_20)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"Random Forest Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_20, y_pred)
print(f"Random Forest Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_20, y_pred)
print(f"Random Forest Test Recall: {recall:.4f}")

# Calculate F1 score
f1 = f1_score(y_test_20, y_pred)
print(f"Random Forest Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_20, y_pred_proba)
print(f"Random Forest Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_20, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"Random Forest Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_20, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Model Building Using random forest Classifier(70:30)**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Initialize the Random Forest model
random_forest_model = RandomForestClassifier()

# Train the model on the training data
random_forest_model.fit(X_train_70, y_train_70)

# Predict on the test set
y_pred = random_forest_model.predict(X_test_30)

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Random Forest Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameter-Random Forest Classifier(70:30)**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns

y_pred_proba = random_forest_model.predict_proba(X_test_30)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"Random Forest Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_30, y_pred)
print(f"Random Forest Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_30, y_pred)
print(f"Random Forest Test Recall: {recall:.4f}")

# Calculate F1 score
f1 = f1_score(y_test_30, y_pred)
print(f"Random Forest Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_30, y_pred_proba)
print(f"Random Forest Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_30, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"Random Forest Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_30, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Model Building using Adaboost classifier(80:20)**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

# Initialize the AdaBoost model
adaboost_model = AdaBoostClassifier()

# Train the model on the training data
adaboost_model.fit(X_train_80, y_train_80)

# Predict on the test set
y_pred = adaboost_model.predict(X_test_20)

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"AdaBoost Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameter-Adaboost Classifier(80:20)**

"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns

y_pred_proba = adaboost_model.predict_proba(X_test_20)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_20, y_pred)
print(f"AdaBoost Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_20, y_pred)
print(f"AdaBoost Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_20, y_pred)
print(f"AdaBoost Test Recall: {recall:.4f}")

# Calculate F1 score
f1 = f1_score(y_test_20, y_pred)
print(f"AdaBoost Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_20, y_pred_proba)
print(f"AdaBoost Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_20, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")


# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"AdaBoost Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_20, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Model Building Using Adaboost Classifier(70:30)**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

# Initialize the AdaBoost model
adaboost_model = AdaBoostClassifier()

# Train the model on the training data
adaboost_model.fit(X_train_70, y_train_70)

# Predict on the test set
y_pred = adaboost_model.predict(X_test_30)

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"AdaBoost Test Accuracy: {accuracy:.4f}")

"""**Evaluation Parameter-Adaboost Classifier(70:30)**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix,f1_score
import matplotlib.pyplot as plt
import seaborn as sns


y_pred_proba = adaboost_model.predict_proba(X_test_30)[:, 1]  # Probability estimates for the positive class

# Calculate accuracy
accuracy = accuracy_score(y_test_30, y_pred)
print(f"AdaBoost Test Accuracy: {accuracy:.4f}")

# Calculate precision
precision = precision_score(y_test_30, y_pred)
print(f"AdaBoost Test Precision: {precision:.4f}")

# Calculate recall
recall = recall_score(y_test_30, y_pred)
print(f"AdaBoost Test Recall: {recall:.4f}")

# Calculate F1 score
f1 = f1_score(y_test_30, y_pred)
print(f"AdaBoost Test F1 Score: {f1:.4f}")

# Calculate AUC-ROC
auc_roc = roc_auc_score(y_test_30, y_pred_proba)
print(f"AdaBoost Test AUC-ROC: {auc_roc:.4f}")

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test_30, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Calculate misclassification rate
misclassification_rate = 1 - accuracy
print(f"AdaBoost Test Misclassification Rate: {misclassification_rate:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_30, y_pred_proba)
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""### Model Performance Comparison Analysis(80:20)

| Model            | Accuracy | Precision | Recall  | F1 Score | AUC-ROC | Misclassification Rate |
|------------------|----------|-----------|---------|----------|---------|------------------------|
| **AdaBoost**     | 0.8235   | 0.8421    | 0.9143  | 0.8767   | 0.9125  | 0.1765                 |
| **Random Forest**| 0.9020   | 0.9167    | 0.9429  | 0.9296   | 0.9750  | 0.0980                 |
| **Naive Bayes**  | 0.8627   | 0.9375    | 0.8571  | 0.8955   | 0.9411  | 0.1373                 |
| **KNN**          | 0.8431   | 0.9655    | 0.8000  | 0.8750   | 0.9643  | 0.1569                 |



1. **Random Forest**: Has the highest accuracy, precision, recall, F1 score, and AUC-ROC.
2. **KNN**: Second highest in most metrics, but again slightly lower than Random Forest.
3. **Naive Bayes**: Good performance, but lower than Random Forest and KNN.
4. **AdaBoost**: Lower performance in most metrics

### Model Performance Comparison Analysis (70:30)



| Model         | Accuracy | Precision | Recall  | F1 Score | AUC-ROC | Misclassification Rate |
|---------------|----------|-----------|---------|----------|---------|------------------------|
| **AdaBoost**       | 0.8289   | 0.8333    | 0.9184  | 0.8738   | 0.9282  | 0.1711                 |
| **Random Forest**  | 0.9474   | 0.9412    | 0.9796  | 0.9600   | 0.9860  | 0.0526                 |
| **Naive Bayes**    | 0.8947   | 0.9184    | 0.9184  | 0.9184   | 0.9637  | 0.1053                 |
| **KNN**            | 0.9211   | 0.9778    | 0.8980  | 0.9362   | 0.9747  | 0.0789                 |


1. **Random Forest**: Highest accuracy, precision, recall, F1 score, and AUC-ROC.
2. **KNN**: Second highest in most metrics, but slightly lower in accuracy and AUC-ROC compared to Random Forest.
3. **Naive Bayes**: Good performance, but lower than Random Forest and KNN.
4. **AdaBoost**: Generally lower performance compared to others.

Random Forest appears to be the best model for diabetes prediction for both data splits (70:30 and 80:20) based on the evaluation metrics

**Fine-Tuning Hyperparameters**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.

**Fine-Tuning Hyperparameters-KNN Classifier(80:20)**
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define the parameter grid for KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'weights': ['uniform', 'distance'],  # Weight function used in prediction
    'p': [1, 2]  # Power parameter for the Minkowski metric (1 for Manhattan, 2 for Euclidean)
}

# Initialize the KNN model
knn_model = KNeighborsClassifier()

# GridSearchCV for KNN
grid_search_knn = GridSearchCV(estimator=knn_model, param_grid=param_grid_knn, cv=5, scoring='accuracy')
grid_search_knn.fit(X_train_80, y_train_80)
print(f"Best parameters for KNN (Grid Search): {grid_search_knn.best_params_}")
print(f"Best accuracy for KNN (Grid Search): {grid_search_knn.best_score_:.4f}")

# RandomizedSearchCV for KNN
random_search_knn = RandomizedSearchCV(estimator=knn_model, param_distributions=param_grid_knn, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_knn.fit(X_train_80, y_train_80)
print(f"Best parameters for KNN (Randomized Search): {random_search_knn.best_params_}")
print(f"Best accuracy for KNN (Randomized Search): {random_search_knn.best_score_:.4f}")

"""**Fine-Tuning Hyperparameters-KNN Classifier(70:30)**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define the parameter grid for KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'weights': ['uniform', 'distance'],  # Weight function used in prediction
    'p': [1, 2]  # Power parameter for the Minkowski metric (1 for Manhattan, 2 for Euclidean)
}

# Initialize the KNN model
knn_model = KNeighborsClassifier()

# GridSearchCV for KNN
grid_search_knn = GridSearchCV(estimator=knn_model, param_grid=param_grid_knn, cv=5, scoring='accuracy')
grid_search_knn.fit(X_train_70, y_train_70)
print(f"Best parameters for KNN (Grid Search): {grid_search_knn.best_params_}")
print(f"Best accuracy for KNN (Grid Search): {grid_search_knn.best_score_:.4f}")

# RandomizedSearchCV for KNN
random_search_knn = RandomizedSearchCV(estimator=knn_model, param_distributions=param_grid_knn, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_knn.fit(X_train_70, y_train_70)
print(f"Best parameters for KNN (Randomized Search): {random_search_knn.best_params_}")
print(f"Best accuracy for KNN (Randomized Search): {random_search_knn.best_score_:.4f}")

"""**Fine-Tuning Hyperparameters-Naive Bayes Classifier(80:20)**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Step 1: Preprocess the data to ensure it is suitable for MultinomialNB
# Scale the data to a range of [0, 1] and then convert it to non-negative integer counts
scaler = MinMaxScaler(feature_range=(0, 1))
X_train_80_scaled = scaler.fit_transform(X_train_80)
X_train_80_scaled = np.round(X_train_80_scaled * 100).astype(int)

# Step 2: Define the parameter grid for Multinomial Naive Bayes
param_grid_nb = {
    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]  # Smoothing parameter
}

# Step 3: Initialize the Naive Bayes model
nb_model = MultinomialNB()

# Step 4: Perform GridSearchCV
grid_search_nb = GridSearchCV(estimator=nb_model, param_grid=param_grid_nb, cv=5, scoring='accuracy')
grid_search_nb.fit(X_train_80_scaled, y_train_80)
print(f"Best parameters for Naive Bayes (Grid Search): {grid_search_nb.best_params_}")
print(f"Best accuracy for Naive Bayes (Grid Search): {grid_search_nb.best_score_:.4f}")

# Step 5: Perform RandomizedSearchCV
random_search_nb = RandomizedSearchCV(estimator=nb_model, param_distributions=param_grid_nb, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_nb.fit(X_train_80_scaled, y_train_80)
print(f"Best parameters for Naive Bayes (Randomized Search): {random_search_nb.best_params_}")
print(f"Best accuracy for Naive Bayes (Randomized Search): {random_search_nb.best_score_:.4f}")

"""**Fine-Tuning Hyperparameters-Naive Bayes Classifier(70:30)**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Step 1: Preprocess the data to ensure it is suitable for MultinomialNB
# Scale the data to a range of [0, 1] and then convert it to non-negative integer counts
scaler = MinMaxScaler(feature_range=(0, 1))
X_train_70_scaled = scaler.fit_transform(X_train_70)
X_train_70_scaled = np.round(X_train_70_scaled * 100).astype(int)

# Step 2: Define the parameter grid for Multinomial Naive Bayes
param_grid_nb = {
    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]  # Smoothing parameter
}

# Step 3: Initialize the Naive Bayes model
nb_model = MultinomialNB()

# Step 4: Perform GridSearchCV
grid_search_nb = GridSearchCV(estimator=nb_model, param_grid=param_grid_nb, cv=5, scoring='accuracy')
grid_search_nb.fit(X_train_70_scaled, y_train_70)
print(f"Best parameters for Naive Bayes (Grid Search): {grid_search_nb.best_params_}")
print(f"Best accuracy for Naive Bayes (Grid Search): {grid_search_nb.best_score_:.4f}")

# Step 5: Perform RandomizedSearchCV
random_search_nb = RandomizedSearchCV(estimator=nb_model, param_distributions=param_grid_nb, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_nb.fit(X_train_70_scaled, y_train_70)
print(f"Best parameters for Naive Bayes (Randomized Search): {random_search_nb.best_params_}")
print(f"Best accuracy for Naive Bayes (Randomized Search): {random_search_nb.best_score_:.4f}")

"""**Fine-Tuning Hyperparameters-Random Forest Classifier(80:20)**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.
"""

from sklearn.ensemble import RandomForestClassifier

# Define the parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# GridSearchCV for Random Forest
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train_80, y_train_80)
print(f"Best parameters for Random Forest (Grid Search): {grid_search_rf.best_params_}")
print(f"Best accuracy for Random Forest (Grid Search): {grid_search_rf.best_score_:.4f}")

# RandomizedSearchCV for Random Forest
random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid_rf, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_rf.fit(X_train_80, y_train_80)
print(f"Best parameters for Random Forest (Randomized Search): {random_search_rf.best_params_}")
print(f"Best accuracy for Random Forest (Randomized Search): {random_search_rf.best_score_:.4f}")

"""**Fine-Tuning Hyperparameters-Random Forest Classifier(70:30)**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics. We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.
"""

from sklearn.ensemble import RandomForestClassifier

# Define the parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# GridSearchCV for Random Forest
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train_70, y_train_70)
print(f"Best parameters for Random Forest (Grid Search): {grid_search_rf.best_params_}")
print(f"Best accuracy for Random Forest (Grid Search): {grid_search_rf.best_score_:.4f}")

# RandomizedSearchCV for Random Forest
random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid_rf, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_rf.fit(X_train_70, y_train_70)
print(f"Best parameters for Random Forest (Randomized Search): {random_search_rf.best_params_}")
print(f"Best accuracy for Random Forest (Randomized Search): {random_search_rf.best_score_:.4f}")

"""**Fine-Tuning Hyperparameters-Adaboost Classifier(80:20)**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics. We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.
"""

from sklearn.ensemble import AdaBoostClassifier

# Define the parameter grid for AdaBoost
param_grid_ab = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1.0, 2.0]
}

# Initialize the AdaBoost model
ab_model = AdaBoostClassifier(random_state=42)

# GridSearchCV for AdaBoost
grid_search_ab = GridSearchCV(estimator=ab_model, param_grid=param_grid_ab, cv=5, scoring='accuracy')
grid_search_ab.fit(X_train_80, y_train_80)
print(f"Best parameters for AdaBoost (Grid Search): {grid_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Grid Search): {grid_search_ab.best_score_:.4f}")

# RandomizedSearchCV for AdaBoost
random_search_ab = RandomizedSearchCV(estimator=ab_model, param_distributions=param_grid_ab, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_ab.fit(X_train_80, y_train_80)
print(f"Best parameters for AdaBoost (Randomized Search): {random_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Randomized Search): {random_search_ab.best_score_:.4f}")

"""**Fine-Tuning Hyperparameters-Adaboost Classifier(70:30)**

We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics. We are required to explore the hyperparameter space for each classifier, utilizing techniques such as grid search or randomized search, to find the optimal combination of parameters that maximizes performance metrics.
"""

from sklearn.ensemble import AdaBoostClassifier

# Define the parameter grid for AdaBoost
param_grid_ab = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1.0, 2.0]
}

# Initialize the AdaBoost model
ab_model = AdaBoostClassifier(random_state=42)

# GridSearchCV for AdaBoost
grid_search_ab = GridSearchCV(estimator=ab_model, param_grid=param_grid_ab, cv=5, scoring='accuracy')
grid_search_ab.fit(X_train_70, y_train_70)
print(f"Best parameters for AdaBoost (Grid Search): {grid_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Grid Search): {grid_search_ab.best_score_:.4f}")

# RandomizedSearchCV for AdaBoost
random_search_ab = RandomizedSearchCV(estimator=ab_model, param_distributions=param_grid_ab, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_ab.fit(X_train_70, y_train_70)
print(f"Best parameters for AdaBoost (Randomized Search): {random_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Randomized Search): {random_search_ab.best_score_:.4f}")

"""**Performance Evaluation**

**Performance Evaluation-Fine tuning hyperparameters-KNN Classifier(80:20)**
"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Retrain KNN with best parameters from GridSearchCV
best_knn_grid = grid_search_knn.best_estimator_
best_knn_grid.fit(X_train_80, y_train_80)

# Retrain KNN with best parameters from RandomizedSearchCV
best_knn_random = random_search_knn.best_estimator_
best_knn_random.fit(X_train_80, y_train_80)

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both models using X_test_20 and y_test_20
metrics_knn_grid = evaluate_model(best_knn_grid, X_test_20, y_test_20)
metrics_knn_random = evaluate_model(best_knn_random, X_test_20, y_test_20)

# Print metrics
print(f"KNN (Grid Search) - Precision: {metrics_knn_grid[0]:.4f}, Recall: {metrics_knn_grid[1]:.4f}, F1: {metrics_knn_grid[2]:.4f}, Accuracy: {metrics_knn_grid[3]:.4f}, Misclassification Rate: {metrics_knn_grid[4]:.4f}")
print(f"KNN (Randomized Search) - Precision: {metrics_knn_random[0]:.4f}, Recall: {metrics_knn_random[1]:.4f}, F1: {metrics_knn_random[2]:.4f}, Accuracy: {metrics_knn_random[3]:.4f}, Misclassification Rate: {metrics_knn_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_knn_grid[5], title='KNN (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_knn_random[5], title='KNN (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_knn_grid, X_test_20, y_test_20)
plot_roc_curve(best_knn_random, X_test_20, y_test_20)

"""**Performance Evaluation-Fine tuning hyperparameters-KNN Classifier(70:30)**"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Retrain KNN with best parameters from GridSearchCV
best_knn_grid = grid_search_knn.best_estimator_
best_knn_grid.fit(X_train_70, y_train_70)

# Retrain KNN with best parameters from RandomizedSearchCV
best_knn_random = random_search_knn.best_estimator_
best_knn_random.fit(X_train_70, y_train_70)

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both models using X_test_30 and y_test_30
metrics_knn_grid = evaluate_model(best_knn_grid, X_test_30, y_test_30)
metrics_knn_random = evaluate_model(best_knn_random, X_test_30, y_test_30)

# Print metrics
print(f"KNN (Grid Search) - Precision: {metrics_knn_grid[0]:.4f}, Recall: {metrics_knn_grid[1]:.4f}, F1: {metrics_knn_grid[2]:.4f}, Accuracy: {metrics_knn_grid[3]:.4f}, Misclassification Rate: {metrics_knn_grid[4]:.4f}")
print(f"KNN (Randomized Search) - Precision: {metrics_knn_random[0]:.4f}, Recall: {metrics_knn_random[1]:.4f}, F1: {metrics_knn_random[2]:.4f}, Accuracy: {metrics_knn_random[3]:.4f}, Misclassification Rate: {metrics_knn_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_knn_grid[5], title='KNN (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_knn_random[5], title='KNN (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_knn_grid, X_test_30, y_test_30)
plot_roc_curve(best_knn_random, X_test_30, y_test_30)

"""**Performance Evaluation-Fine tuning hyperparameters-Naive Bayes Classifier(80:20)**"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Retrain Naive Bayes models with best parameters from GridSearchCV and RandomizedSearchCV
best_nb_grid = grid_search_nb.best_estimator_
best_nb_grid.fit(X_train_80_scaled, y_train_80)

best_nb_random = random_search_nb.best_estimator_
best_nb_random.fit(X_train_80_scaled, y_train_80)

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both models using X_test_20 and y_test_20
metrics_nb_grid = evaluate_model(best_nb_grid, X_test_20, y_test_20)
metrics_nb_random = evaluate_model(best_nb_random, X_test_20, y_test_20)

# Print metrics
print(f"Naive Bayes (Grid Search) - Precision: {metrics_nb_grid[0]:.4f}, Recall: {metrics_nb_grid[1]:.4f}, F1: {metrics_nb_grid[2]:.4f}, Accuracy: {metrics_nb_grid[3]:.4f}, Misclassification Rate: {metrics_nb_grid[4]:.4f}")
print(f"Naive Bayes (Randomized Search) - Precision: {metrics_nb_random[0]:.4f}, Recall: {metrics_nb_random[1]:.4f}, F1: {metrics_nb_random[2]:.4f}, Accuracy: {metrics_nb_random[3]:.4f}, Misclassification Rate: {metrics_nb_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_nb_grid[5], title='Naive Bayes (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_nb_random[5], title='Naive Bayes (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_nb_grid, X_test_20, y_test_20)
plot_roc_curve(best_nb_random, X_test_20, y_test_20)

"""**Performance Evaluation-Fine tuning hyperparameters-Naive Bayes Classifier(70:30)**"""

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Retrain Naive Bayes models with best parameters from GridSearchCV and RandomizedSearchCV
best_nb_grid = grid_search_nb.best_estimator_
best_nb_grid.fit(X_train_70_scaled, y_train_70)

best_nb_random = random_search_nb.best_estimator_
best_nb_random.fit(X_train_70_scaled, y_train_70)

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both models using X_test_30 and y_test_30
metrics_nb_grid = evaluate_model(best_nb_grid, X_test_30, y_test_30)
metrics_nb_random = evaluate_model(best_nb_random, X_test_30, y_test_30)

# Print metrics
print(f"Naive Bayes (Grid Search) - Precision: {metrics_nb_grid[0]:.4f}, Recall: {metrics_nb_grid[1]:.4f}, F1: {metrics_nb_grid[2]:.4f}, Accuracy: {metrics_nb_grid[3]:.4f}, Misclassification Rate: {metrics_nb_grid[4]:.4f}")
print(f"Naive Bayes (Randomized Search) - Precision: {metrics_nb_random[0]:.4f}, Recall: {metrics_nb_random[1]:.4f}, F1: {metrics_nb_random[2]:.4f}, Accuracy: {metrics_nb_random[3]:.4f}, Misclassification Rate: {metrics_nb_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_nb_grid[5], title='Naive Bayes (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_nb_random[5], title='Naive Bayes (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_nb_grid, X_test_30, y_test_30)
plot_roc_curve(best_nb_random, X_test_30, y_test_30)

"""**Performance Evaluation-Fine tuning hyperparameters-Random Forest Classifier(80:20)**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Retrain Random Forest models with best parameters from GridSearchCV and RandomizedSearchCV
best_rf_grid = grid_search_rf.best_estimator_
best_rf_grid.fit(X_train_80, y_train_80)

best_rf_random = random_search_rf.best_estimator_
best_rf_random.fit(X_train_80, y_train_80)

# Verify the best parameters
print(f"Best parameters for Random Forest (Grid Search): {grid_search_rf.best_params_}")
print(f"Best parameters for Random Forest (Randomized Search): {random_search_rf.best_params_}")

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both models using X_test_20 and y_test_20
metrics_rf_grid = evaluate_model(best_rf_grid, X_test_20, y_test_20)
metrics_rf_random = evaluate_model(best_rf_random, X_test_20, y_test_20)

# Print metrics
print(f"Random Forest (Grid Search) - Precision: {metrics_rf_grid[0]:.4f}, Recall: {metrics_rf_grid[1]:.4f}, F1: {metrics_rf_grid[2]:.4f}, Accuracy: {metrics_rf_grid[3]:.4f}, Misclassification Rate: {metrics_rf_grid[4]:.4f}")
print(f"Random Forest (Randomized Search) - Precision: {metrics_rf_random[0]:.4f}, Recall: {metrics_rf_random[1]:.4f}, F1: {metrics_rf_random[2]:.4f}, Accuracy: {metrics_rf_random[3]:.4f}, Misclassification Rate: {metrics_rf_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_rf_grid[5], title='Random Forest (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_rf_random[5], title='Random Forest (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    # Random Forest models support predict_proba method
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_rf_grid, X_test_20, y_test_20)
plot_roc_curve(best_rf_random, X_test_20, y_test_20)

# Additional diagnostics: Feature importances
print("Feature Importances (Grid Search):")
print(best_rf_grid.feature_importances_)

print("Feature Importances (Randomized Search):")
print(best_rf_random.feature_importances_)

# If necessary, fit both models on a different subset of the test set or use cross-validation scores

"""**Performance Evaluation-Fine tuning hyperparameters-Random Forest Classifier(70:30)**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Retrain Random Forest models with best parameters from GridSearchCV and RandomizedSearchCV
best_rf_grid = grid_search_rf.best_estimator_
best_rf_grid.fit(X_train_70, y_train_70)

best_rf_random = random_search_rf.best_estimator_
best_rf_random.fit(X_train_70, y_train_70)

# Verify the best parameters
print(f"Best parameters for Random Forest (Grid Search): {grid_search_rf.best_params_}")
print(f"Best parameters for Random Forest (Randomized Search): {random_search_rf.best_params_}")

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both models using X_test_30 and y_test_30
metrics_rf_grid = evaluate_model(best_rf_grid, X_test_30, y_test_30)
metrics_rf_random = evaluate_model(best_rf_random, X_test_30, y_test_30)

# Print metrics
print(f"Random Forest (Grid Search) - Precision: {metrics_rf_grid[0]:.4f}, Recall: {metrics_rf_grid[1]:.4f}, F1: {metrics_rf_grid[2]:.4f}, Accuracy: {metrics_rf_grid[3]:.4f}, Misclassification Rate: {metrics_rf_grid[4]:.4f}")
print(f"Random Forest (Randomized Search) - Precision: {metrics_rf_random[0]:.4f}, Recall: {metrics_rf_random[1]:.4f}, F1: {metrics_rf_random[2]:.4f}, Accuracy: {metrics_rf_random[3]:.4f}, Misclassification Rate: {metrics_rf_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_rf_grid[5], title='Random Forest (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_rf_random[5], title='Random Forest (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    # Random Forest models support predict_proba method
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_rf_grid, X_test_30, y_test_30)
plot_roc_curve(best_rf_random, X_test_30, y_test_30)

# Additional diagnostics: Feature importances
print("Feature Importances (Grid Search):")
print(best_rf_grid.feature_importances_)

print("Feature Importances (Randomized Search):")
print(best_rf_random.feature_importances_)

# If necessary, fit both models on a different subset of the test set or use cross-validation scores

"""**Performance Evaluation-Fine tuning hyperparameters-Adaboost Classifier(80:20)**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Define the parameter grid for AdaBoost
param_grid_ab = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1.0, 2.0]
}

# Initialize the AdaBoost model
ab_model = AdaBoostClassifier(random_state=42)

# GridSearchCV for AdaBoost
grid_search_ab = GridSearchCV(estimator=ab_model, param_grid=param_grid_ab, cv=5, scoring='accuracy')
grid_search_ab.fit(X_train_80, y_train_80)
print(f"Best parameters for AdaBoost (Grid Search): {grid_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Grid Search): {grid_search_ab.best_score_:.4f}")

# RandomizedSearchCV for AdaBoost
random_search_ab = RandomizedSearchCV(estimator=ab_model, param_distributions=param_grid_ab, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_ab.fit(X_train_80, y_train_80)
print(f"Best parameters for AdaBoost (Randomized Search): {random_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Randomized Search): {random_search_ab.best_score_:.4f}")

# Retrieve the best models
best_ab_grid = grid_search_ab.best_estimator_
best_ab_random = random_search_ab.best_estimator_

# Fit the models
best_ab_grid.fit(X_train_80, y_train_80)
best_ab_random.fit(X_train_80, y_train_80)

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both AdaBoost models
metrics_ab_grid = evaluate_model(best_ab_grid, X_test_20, y_test_20)
metrics_ab_random = evaluate_model(best_ab_random, X_test_20, y_test_20)

# Print metrics
print(f"AdaBoost (Grid Search) - Precision: {metrics_ab_grid[0]:.4f}, Recall: {metrics_ab_grid[1]:.4f}, F1: {metrics_ab_grid[2]:.4f}, Accuracy: {metrics_ab_grid[3]:.4f}, Misclassification Rate: {metrics_ab_grid[4]:.4f}")
print(f"AdaBoost (Randomized Search) - Precision: {metrics_ab_random[0]:.4f}, Recall: {metrics_ab_random[1]:.4f}, F1: {metrics_ab_random[2]:.4f}, Accuracy: {metrics_ab_random[3]:.4f}, Misclassification Rate: {metrics_ab_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_ab_grid[5], title='AdaBoost (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_ab_random[5], title='AdaBoost (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    # AdaBoost models support predict_proba method
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_ab_grid, X_test_20, y_test_20)
plot_roc_curve(best_ab_random, X_test_20, y_test_20)

"""**Performance Evaluation-Fine tuning hyperparameters-Adaboost Classifier(70:30)**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Define the parameter grid for AdaBoost
param_grid_ab = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1.0, 2.0]
}

# Initialize the AdaBoost model
ab_model = AdaBoostClassifier(random_state=42)

# GridSearchCV for AdaBoost
grid_search_ab = GridSearchCV(estimator=ab_model, param_grid=param_grid_ab, cv=5, scoring='accuracy')
grid_search_ab.fit(X_train_70, y_train_70)
print(f"Best parameters for AdaBoost (Grid Search): {grid_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Grid Search): {grid_search_ab.best_score_:.4f}")

# RandomizedSearchCV for AdaBoost
random_search_ab = RandomizedSearchCV(estimator=ab_model, param_distributions=param_grid_ab, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search_ab.fit(X_train_70, y_train_70)
print(f"Best parameters for AdaBoost (Randomized Search): {random_search_ab.best_params_}")
print(f"Best accuracy for AdaBoost (Randomized Search): {random_search_ab.best_score_:.4f}")

# Retrieve the best models
best_ab_grid = grid_search_ab.best_estimator_
best_ab_random = random_search_ab.best_estimator_

# Fit the models
best_ab_grid.fit(X_train_70, y_train_70)
best_ab_random.fit(X_train_70, y_train_70)

# Evaluate on test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    accuracy = accuracy_score(y_test, y_pred)
    misclassification_rate = 1 - accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)

    return precision, recall, f1, accuracy, misclassification_rate, conf_matrix

# Evaluate both AdaBoost models
metrics_ab_grid = evaluate_model(best_ab_grid, X_test_30, y_test_30)
metrics_ab_random = evaluate_model(best_ab_random, X_test_30, y_test_30)

# Print metrics
print(f"AdaBoost (Grid Search) - Precision: {metrics_ab_grid[0]:.4f}, Recall: {metrics_ab_grid[1]:.4f}, F1: {metrics_ab_grid[2]:.4f}, Accuracy: {metrics_ab_grid[3]:.4f}, Misclassification Rate: {metrics_ab_grid[4]:.4f}")
print(f"AdaBoost (Randomized Search) - Precision: {metrics_ab_random[0]:.4f}, Recall: {metrics_ab_random[1]:.4f}, F1: {metrics_ab_random[2]:.4f}, Accuracy: {metrics_ab_random[3]:.4f}, Misclassification Rate: {metrics_ab_random[4]:.4f}")

# Plot confusion matrix
def plot_confusion_matrix(cm, title='Confusion Matrix'):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(metrics_ab_grid[5], title='AdaBoost (Grid Search) Confusion Matrix')
plot_confusion_matrix(metrics_ab_random[5], title='AdaBoost (Randomized Search) Confusion Matrix')

# ROC Curve and AUC
def plot_roc_curve(model, X_test, y_test):
    # AdaBoost models support predict_proba method
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(best_ab_grid, X_test_30, y_test_30)
plot_roc_curve(best_ab_random, X_test_30, y_test_30)

"""**Comparison and Analysis**

Model Comparision and Analysis after fine-tuning the hyperparamters (data split 80:20) using Grid Search CV

| Model          | Precision | Recall | F1 Score | Accuracy | Misclassification Rate |
|----------------|-----------|--------|----------|----------|------------------------|
| **AdaBoost**   | 0.8616    | 0.8431 | 0.8471   | 0.8431   | 0.1569                 |
| **Random Forest** | 0.9219 | 0.9216 | 0.9200   | 0.9216   | 0.0784                 |
| **Naive Bayes** | 0.8856   | 0.8627 | 0.8498   | 0.8627   | 0.1373                 |
| **KNN**        | 0.8994    | 0.8824 | 0.8853   | 0.8824   | 0.1176                 |

Model Comparision and Analysis after fine-tuning the hyperparamters (data split 80:20) using Random Search CV


| Model          | Precision | Recall | F1 Score | Accuracy | Misclassification Rate |
|----------------|-----------|--------|----------|----------|------------------------|
| **AdaBoost**   | 0.8616    | 0.8431 | 0.8471   | 0.8431   | 0.1569                 |
| **Random Forest** | 0.9219 | 0.9216 | 0.9200   | 0.9216   | 0.0784                 |
| **Naive Bayes** | 0.8856   | 0.8627 | 0.8498   | 0.8627   | 0.1373                 |
| **KNN**        | 0.9045    | 0.8627 | 0.8674   | 0.8627   | 0.1373                 |

1. Random Forest has the highest accuracy 0.9219 and lowest misclassification rate 0.0784 in both Grid Search and Randomized Search, showing it is more useful for diabetes risk prediction.

2. Naive Bayes and Adaboost has a higher misclassification rates it might miss more cases of diabetes risk.

3. KNN also has good Accuracy but other parameters lower

Model Comparision and Analysis after fine-tuning the hyperparameters (data split 70:30) using Grid Search CV

| Model        | Accuracy | Precision | Recall | F1 Score | Misclassification Rate |
|--------------|----------|-----------|--------|----------|------------------------|
| AdaBoost     | 0.8553   | 0.8609    | 0.8553 | 0.8493   | 0.1447                 |
| Random Forest| 0.9474   | 0.9479    | 0.9474 | 0.9469   | 0.0526                 |
| Naive Bayes  | 0.8026   | 0.8489    | 0.8026 | 0.7778   | 0.1974                 |
| KNN          | 0.9211   | 0.9284    | 0.9211 | 0.9221   | 0.0789                 |

Model Comparision and Analysis after fine-tuning the hyperparameters (data split 70:30) using Random Search CV


| Model        | Accuracy | Precision | Recall | F1 Score | Misclassification Rate |
|--------------|----------|-----------|--------|----------|------------------------|
| AdaBoost     | 0.8553   | 0.8609    | 0.8553 | 0.8493   | 0.1447                 |
| Random Forest| 0.9342   | 0.9356    | 0.9342 | 0.9333   | 0.0658                 |
| Naive Bayes  | 0.8026   | 0.8489    | 0.8026 | 0.7778   | 0.1974                 |
| KNN          | 0.9079   | 0.9269    | 0.9079 | 0.9096   | 0.0921                 |

1. Random Forest with Grid Search achieved the highest accuracy, Precison and Recall, indicating it is the most useful and useful model for Diabetis Risk Prediction.

2. Naive Bayes has the lowest performance, with higher misclassification rates, suggesting that it might not be the most suitable model for predicting diabetes risk. Also, its lower recall indicates that it may fail to correctly identify individuals at risk, which is critical in diabetes risk prediction.

3. AdaBoost also shows good performance, and consistent metrics with both techniques of hyperparamter tuning.

4. KNN also has good accuracy and other parameters after RF.

*Random Forest has best performance across multiple metrics so it is the most suitable model for predicting diabetes risk before after hyperparameter tuning*
"""