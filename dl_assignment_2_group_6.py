# -*- coding: utf-8 -*-
"""DL_assignment_2_group_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eAxnXNvLNpgM9Izo6rtn8eRipdJK7Rfj

## Group No : 6

## Group Member Names:
1. Souvik Nag
2. Sudeet Mhatre
3. Soumen Upadhay
4. Apoorva Belsare

## Journal used for the implemetation
**Journal title:**
Eff-AQI: An Efficient CNN-Based Model for Air Pollution Estimation: A Study Case in India

**Authors:**

Sapdo Utomo sapdo.utomo@gmail.com National Chung Cheng University Chiayi County, Taiwan (R.O.C.)

Chang Chun Hao howard31124@gmail.com National Chung Cheng University Chiayi County, Taiwan (R.O.C.)

 Adarsh Rouniyar adarsh@csie.io National Chung Cheng University Chiayi County, Taiwan (R.O.C.)

Tang Kai Chun 4685231gf@gmail.com National Chung Cheng University Chiayi County, Taiwan (R.O.C.)

 Pao-Ann Hsiung∗ pahsiung@ccu.edu.tw National Chung Cheng University Chiayi County, Taiwan (R.O.C.)


**Journal Name:**  Eff-AQI: An Efficient CNN-Based Model for Air Pollution Estimation: A Study Case in India

**Year:** 06 September 2023

▪ The paper's objectives :     This study focuses on classifying Air Quality Index (AQI) using an image dataset comprising 12,240 images captured from various environments.Enabling the development of an image-based pollution classification system.

▪ The methodologies/algorithms implemented : A Convolutional Neural Network (CNN) model is used to extract meaningful features from images and classify AQI levels.

▪ The significance of the stud : The proposed model offers an automated way to assess air quality using images, which could be beneficial for real-time pollution tracking and environmental analysis.

# 1. Import the required libraries
"""

##---------Type the code below this line------------------##

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
import random

from cv2 import imread, resize
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.metrics import Precision, Recall

"""# 2. Data Acquisition

For the problem identified by you, students have to find the data source themselves from any data source.

Provide the URL of the data used. https://www.kaggle.com/datasets/adarshrouniyar/air-pollution-image-dataset-from-india-and-nepal

Write Code for converting the above downloaded data into a form suitable for DL


"""

##---------Type the code below this line------------------##

!pip install opendatasets
import opendatasets as od

od.download("https://www.kaggle.com/datasets/adarshrouniyar/air-pollution-image-dataset-from-india-and-nepal") #to download dataset from Kaggle ( using Api key )

current_directory = os.getcwd()
print("Current Working Directory:", current_directory)

# print first 5 rows
df = pd.read_csv('/content/air-pollution-image-dataset-from-india-and-nepal/Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_Nep_AQI_Dataset.csv')
df.head(5)

# print last 5 rows
df.tail(5)

"""# 3. Data Preparation

Perform the data prepracessing that is required for the data that you have downloaded.


This stage depends on the dataset that is used.
"""

##---------Type the code below this line------------------##
df['aqi_id'] = df['AQI_Class'].map({'a_Good': 0, 'b_Moderate': 1, 'c_Unhealthy_for_Sensitive_Groups': 2, 'd_Unhealthy': 3, 'e_Very_Unhealthy': 4, 'f_Severe': 5})
df

SIZE = 112
X, y = [], []

# Dictionary to map class labels to numeric ids
label_map = {
    'a_Good': 0,
    'b_Moderate': 1,
    'c_Unhealthy_for_Sensitive_Groups': 2,
    'd_Unhealthy': 3,
    'e_Very_Unhealthy': 4,
    'f_Severe': 5
}

for label, label_id in label_map.items():
    image_folder = f"/content/air-pollution-image-dataset-from-india-and-nepal/Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_NEP/{label}/"
    image_files = os.listdir(image_folder)
    for fname in image_files:
        img = imread(image_folder + fname)
        img_resized = resize(img, (SIZE, SIZE))
        X.append(img_resized)
        y.append(label_id)

X = np.array(X)
y = np.array(y)

# Convert y to categorical (one-hot encoding)
y = to_categorical(y, num_classes=6)

## Split the data into training set and testing set
##---------Type the code below this line------------------##
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

## Identify the target variables.
##---------Type the code below this line------------------##

# Convert one-hot encoded y back to class indices
y_labels = np.argmax(y, axis=1)
class_names = ['Good', 'Moderate', 'Unhealthy_Sensitive', 'Unhealthy', 'Very_Unhealthy', 'Severe']

fig, axes = plt.subplots(2, 5, figsize=(12, 5))

for i, ax in enumerate(axes.flat):
    class_id = i % 6  # Pick different categories cyclically (0 to 5)

    # Get all indices of this class
    class_indices = np.where(y_labels == class_id)[0]

    if len(class_indices) > 0:  # Ensure class has samples
        img_index = random.choice(class_indices)  # Pick a random image
        ax.imshow(X[img_index])
        ax.set_title(f"Class: {class_names[class_id]}")
        ax.axis("off")

plt.tight_layout()
plt.show()

"""## 4. Deep Neural Network Architecture

## 4.1 Design the architecture that you will be using

* CNN / RNN / Transformer as per the journal referenced
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Feature Extraction Block (acts as the pre-trained model)
feature_extractor = models.Sequential([
    layers.Conv2D(32, (3,3), padding='same', input_shape=(112, 112, 3)),
    layers.LeakyReLU(alpha=0.1),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(64, (3,3), padding='same'),
    layers.LeakyReLU(alpha=0.1),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(128, (3,3), padding='same'),
    layers.LeakyReLU(alpha=0.1),
    layers.MaxPooling2D((2,2))
])

# Full Model
model = models.Sequential([
    feature_extractor,
    layers.GlobalAveragePooling2D(),

    layers.Dense(1024),
    layers.LeakyReLU(alpha=0.1),
    layers.Dropout(0.5),

    layers.Dense(512),
    layers.LeakyReLU(alpha=0.1),
    layers.Dropout(0.5),

    layers.Dense(6, activation='softmax')  # 6 output classes
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy',
              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Model Summary
model.summary()

"""## 4.2 DNN Report

Report the following and provide justification for the same.

* Number of layers :

Feature Extraction Block: 9 layers (3 Conv2D, 3 LeakyReLU, 3 MaxPooling)
Classification Block: 8 layers (1 GAP, 2 Dense, 2 LeakyReLU, 2 Dropout, 1 Dense)
Total: 16 layers


* Number of units in each layer :

Conv2D: 32, 64, 128 filters

* Total number of trainable parameters :

Total params: 753,222
Trainable params: 753,222
Non-trainable params: 0

# 5. Training the model
"""

# Configure the training, by using appropriate optimizers, regularizations and loss functions
##---------Type the code below this line------------------##
# Train the model
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=20,
                    batch_size=32,
                    verbose=1,
                    callbacks=[early_stopping])

"""# 6. Test the model

"""

##---------Type the code below this line------------------##
test_results = model.evaluate(X_test, y_test, verbose=1)

# Unpack values correctly
test_loss, test_accuracy, test_precision, test_recall = test_results

# Print the results
print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")

"""# 7. Report the result

1. Plot the training and validation accuracy history.
2. Plot the training and validation loss history.
3. Report the testing accuracy and loss.
4. Show Confusion Matrix for testing dataset.
5. Report values for preformance study metrics like accuracy, precision, recall, F1 Score.

"""

##---------Type the code below this line------------------##
#Plot Training & Validation Accuracy
import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.show()

#Plot Training & Validation Loss

plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.show()

# Evaluate the model on the test dataset
test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=1)
print(f"\nTest Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")


import seaborn as sns
from sklearn.metrics import confusion_matrix

# Get predictions on the test set
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions to class labels
y_true = np.argmax(y_test, axis=1)  # Convert one-hot encoded true labels to class labels

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import classification_report

# Generate classification report
report = classification_report(y_true, y_pred_classes, target_names=class_names)
print(report)

"""###  **Model Performance Results**  

**Test Accuracy:** 0.9257

**Test Loss:** 0.2379

### NOTE


All Late Submissions will incur a <b>penalty of -2 marks </b>. So submit your assignments on time.

Good Luck
"""